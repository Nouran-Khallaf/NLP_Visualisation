{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCREL/Session1_Visualisation_and_Summarisation/blob/main/NoteBooks/3_Text_Summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoU4vPdz0htk"
      },
      "source": [
        "## Tutorial: Building a Text Summarizer\n",
        "![Comparison Diagram](https://raw.githubusercontent.com/Nouran-Khallaf/NLP_Visualisation/main/static/images/Extractive-vs-Abstractive-Summarization.png)\n",
        "*Diagram 1: Comparative Analysis of summarisation Techniques*\n",
        "\n",
        "\n",
        "### Introduction\n",
        "This notebook demonstrates how to build a text summariser using  Multi-document Summaries Corpora (2011 and 2013) dataset.\n",
        "The dataset is derived from publicly available WikiNews (http://www.wikinews.org/) English texts. The source texts were under CC Attribution Licence V2.5 (http://creativecommons.org/licenses/by/2.5/). Texts in other languages have been translated by native speakers of each language.\n",
        "Languages included:\n",
        "- Arabic\n",
        "- Czech\n",
        "- English\n",
        "- French\n",
        "- Greek\n",
        "- Hebrew\n",
        "- Hindi\n",
        "- Chinese (2013)\n",
        "- Romanian (2013)\n",
        "- Spanish (2013)\n",
        "\n",
        "For more information, please visit [Multi-document Summaries Corpora](https://www.lancaster.ac.uk/staff/elhaj/corpora.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extractive summarisation**:\n",
        "Extractive summarisation involves selecting key sentences or phrases from the original text and concatenating them to create a summary. The goal is to extract the most important information while maintaining the original context. This method does not generate new sentences but rather identifies and extracts the most relevant parts of the text.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Abstractive summarisation**:\n",
        "Abstractive summarisation, on the other hand, involves generating new sentences that convey the most important information from the original text. This method goes beyond simply extracting key phrases and instead uses techniques like natural language generation to create a coherent and concise summary that may include paraphrasing and restructuring of information.\n",
        "\n",
        "![Comparison Diagram](https://raw.githubusercontent.com/Nouran-Khallaf/NLP_Visualisation/main/static/images/extractive.webp)![Comparison Diagram](https://raw.githubusercontent.com/Nouran-Khallaf/NLP_Visualisation/main/static/images/abstractive.png)\n",
        "*Diagram 2: Analysis of Extractive summarisation *\n",
        "*Diagram 3: Analysis of Abstractive summarisation *\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KxVz67fq0sUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install plotly\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install tabulate\n",
        "!pip install summa"
      ],
      "metadata": {
        "id": "Vju1fXIT1qXT",
        "outputId": "e990d0dd-11b6-422d-8963-8aa1d2625764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Prepare Data\n",
        "We will load the dataset and clean it. For demonstration purposes."
      ],
      "metadata": {
        "id": "DLAAjFXM9ad7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1GSRJgJaRzb1OauKtHdwNoF4UnkKAt9cK'\n",
        "output = 'Multilingual-Dataset.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "6hiCPo0m1rJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "if zipfile.is_zipfile(output):\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir('/content/'))\n",
        "else:\n",
        "    print(\"Downloaded file is not a zip file.\")"
      ],
      "metadata": {
        "id": "dKtQxZMr1yCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Data** **Prepration**\n",
        "we need to organize the text files in the `SourceTexts` directory into subdirectories based on language identifiers extracted from the filenames. It dynamically identifies unique language codes, creates corresponding language-specific folders, and moves each file to its appropriate folder, thereby streamlining file management and access based on language.\n",
        "\n",
        "- Arabic\n",
        "- Czech\n",
        "- English\n",
        "- French\n",
        "- Greek\n",
        "- Hebrew\n",
        "- Hindi"
      ],
      "metadata": {
        "id": "6ery_95JqxUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source directory\n",
        "source_dir = 'SourceTexts'\n",
        "\n",
        "# List all files in the source directory\n",
        "files = os.listdir(source_dir)\n",
        "\n",
        "# Create a set to store unique language identifiers\n",
        "languages = set()\n",
        "\n",
        "# Identify unique languages from filenames\n",
        "for file in files:\n",
        "    language = file.split('.')[1].lower()  # Normalize to lowercase\n",
        "    languages.add(language)\n",
        "\n",
        "# Create folders for each language and move corresponding files\n",
        "for language in languages:\n",
        "    # Create the language folder if it doesn't exist\n",
        "    language_dir = os.path.join(source_dir, language)\n",
        "    os.makedirs(language_dir, exist_ok=True)\n",
        "\n",
        "    # Move files into the language folder\n",
        "    for file in files:\n",
        "        if file.split('.')[1].lower() == language:  # Normalize to lowercase\n",
        "            shutil.move(os.path.join(source_dir, file), os.path.join(language_dir, file))\n",
        "\n",
        "print(\"Files have been organized into folders.\")\n"
      ],
      "metadata": {
        "id": "LHwG8HdB3YaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we combine the content of text files from the `SourceTexts` directory into a single Pandas DataFrame, with each file's content stored as a row in the DataFrame. It then saves the DataFrame to a CSV file.\n",
        "\n",
        "1. **Import Required Modules**:\n",
        "   - The script imports the `os` module for directory and file operations and the `pandas` module for creating and manipulating the DataFrame."
      ],
      "metadata": {
        "id": "y1kdy1KXtfHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NepzRZDjtgpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Define the Function to Combine Files**:\n",
        "   - A function named `combine_files_to_dataframe` is defined. This function takes a directory path as input and returns a DataFrame containing the file names and their contents.\n",
        "- **Initialize an Empty List**: An empty list named `data` is created to store the file names and their contents.\n",
        "- **Loop Through Files**: The function loops through all files in the specified directory using `os.listdir()`.\n",
        "- **Construct Full File Path**: For each file, the full path is constructed using `os.path.join()`.\n",
        "- **Check if it’s a File**: The script ensures the path is a file using `os.path.isfile()`.\n",
        "- **Read File Content**: If it’s a file, the script reads its content and appends a dictionary containing the file name (`Title`) and its content (`Content`) to the `data` list.\n",
        "- **Create a DataFrame**: Finally, a Pandas DataFrame is created from the `data` list and returned."
      ],
      "metadata": {
        "id": "He6MLZTWtlUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_files_to_dataframe(directory_path):\n",
        "       # Initialize an empty list to store the data\n",
        "       data = []\n",
        "\n",
        "       # Loop through all files in the directory\n",
        "       for file_name in os.listdir(directory_path):\n",
        "           # Construct full file path\n",
        "           file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "           # Ensure it's a file\n",
        "           if os.path.isfile(file_path):\n",
        "               # Read the content of each file\n",
        "               with open(file_path, 'r') as file:\n",
        "                   content = file.read()\n",
        "                   # Append the file name and content to the data list\n",
        "                   data.append({'Title': file_name, 'Content': content.strip()})\n",
        "\n",
        "       # Create a DataFrame from the data list\n",
        "       df = pd.DataFrame(data)\n",
        "       return df"
      ],
      "metadata": {
        "id": "nLuQuiQltps_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Specify the Directory Path**:\n",
        "   - The directory containing the files is specified. In this example, the directory path is `/content/SourceTexts/english`."
      ],
      "metadata": {
        "id": "5QT8Bln6t-I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/SourceTexts/english' ## here you can change the language"
      ],
      "metadata": {
        "id": "sSNFdIR1t_pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Combine Files into a DataFrame**:\n",
        "   - The `combine_files_to_dataframe` function is called with the specified directory path, and the resulting DataFrame is stored in the variable `df`."
      ],
      "metadata": {
        "id": "P_IU-C7auL5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = combine_files_to_dataframe(directory_path)"
      ],
      "metadata": {
        "id": "4V6C6E63uOug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Display the DataFrame**:\n",
        "   - The DataFrame is printed to the console to display the combined content of the files."
      ],
      "metadata": {
        "id": "qQYOjKmtuRVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "uwnQd0yPuQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "6. **Save the DataFrame to a CSV File**:\n",
        "   - The DataFrame is saved to a CSV file named `combined_files.csv` without the index using the `to_csv` method."
      ],
      "metadata": {
        "id": "8lhJccJ0uXWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('combined_files.csv', index=False)"
      ],
      "metadata": {
        "id": "hXhQ30hOueJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "> The following code cell is designed to perform the combined operations described above. Let's execute it to see the results.\n",
        "---"
      ],
      "metadata": {
        "id": "WN4pRZj7u0pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def combine_files_to_dataframe(directory_path):\n",
        "    # Initialize an empty list to store the data\n",
        "    data = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        # Construct full file path\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "        # Ensure it's a file\n",
        "        if os.path.isfile(file_path):\n",
        "            # Read the content of each file\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                # Append the file name and content to the data list\n",
        "                data.append({'Title': file_name, 'Content': content.strip()})\n",
        "\n",
        "    # Create a DataFrame from the data list\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = '/content/SourceTexts/english'\n",
        "\n",
        "# Combine the files into a DataFrame\n",
        "df = combine_files_to_dataframe(directory_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "df.to_csv('combined_files.csv', index=False)"
      ],
      "metadata": {
        "id": "ufC-8Qqu18K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Cleaning\n",
        "\n",
        "Enhances the readability and usability of text data by:\n",
        "1. Removing HTML tags using the `BeautifulSoup` library.\n",
        "2. Cleaning the text with regular expressions to replace multiple spaces, remove standalone numbers, and trim whitespace.\n",
        "3. Loading the original text data from a CSV file into a DataFrame.\n",
        "4. Applying the text cleaning process to the 'Content' column and storing the cleaned text in a new column named 'clean_content'."
      ],
      "metadata": {
        "id": "D0jQvrfBvd_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "L6oCkvDXwNQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Define a Function to Clean HTML**:\n",
        "   - A function named `clean_html` is defined to remove HTML tags from a string. It uses `BeautifulSoup` to parse the HTML and extract the text content.\n",
        "- **Parameter**: `raw_html` - The raw HTML string to be cleaned.\n",
        "- **Processing**: `BeautifulSoup` parses the HTML and the `get_text()` method extracts the text content.\n",
        "- **Return**: The cleaned text without HTML tags."
      ],
      "metadata": {
        "id": "p5PPKuh_wjCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(raw_html):\n",
        "       \"\"\"Remove HTML tags from a string\"\"\"\n",
        "       soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "       return soup.get_text()"
      ],
      "metadata": {
        "id": "bLpxCazWwlVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Define a Function to Clean Text**:\n",
        "  - A function named `clean_text` is defined to clean the text by removing unwanted patterns and extra spaces. It first removes HTML tags using the `clean_html` function and then applies various regular expressions to clean the text further.\n",
        "- **Parameter**: `text` - The text string to be cleaned.\n",
        "- **Processing**:\n",
        "     - `clean_html(text)`: Removes HTML tags.\n",
        "     - `re.sub(r'\\s+', ' ', text)`: Replaces multiple spaces and newlines with a single space.\n",
        "     - `re.sub(r'\\b\\d+\\b', '', text)`: Removes standalone numbers (optional, based on specific needs).\n",
        "     - `text.strip()`: Removes leading and trailing spaces.\n",
        "- **Return**: The cleaned text."
      ],
      "metadata": {
        "id": "trJlxZ0aw2Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "       \"\"\"Remove unwanted text patterns and clean the text\"\"\"\n",
        "       text = clean_html(text)\n",
        "       text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
        "       text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers if needed\n",
        "       text = text.strip()  # Remove leading/trailing spaces\n",
        "       return text"
      ],
      "metadata": {
        "id": "UzR-XxpOwXw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Load the DataFrame**:\n",
        "   - The script loads a DataFrame from a CSV file named `combined_files.csv` using the `pandas` library.\n"
      ],
      "metadata": {
        "id": "kHu3xlhkxROP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_files.csv')"
      ],
      "metadata": {
        "id": "8d8j1Ji6wess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Apply Preprocessing to the 'Content' Column**:\n",
        "   - The script applies the `clean_text` function to each entry in the 'Content' column of the DataFrame, creating a new column 'clean_content' with the cleaned text.\n",
        "- **Processing**: The `apply` method is used to apply the `clean_text` function to each row in the 'Content' column.\n",
        "- **Result**: A new column 'clean_content' is added to the DataFrame containing the cleaned text."
      ],
      "metadata": {
        "id": "W_pmFCQxxWeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['Content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "N-l4X1mcxapj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "> The following code cell is designed to perform the combined operations described above. Let's execute it to see the results.\n",
        "---"
      ],
      "metadata": {
        "id": "j-kfX5fpxp4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    \"\"\"Remove HTML tags from a string\"\"\"\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted text patterns and clean the text\"\"\"\n",
        "    text = clean_html(text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers if needed\n",
        "    text = text.strip()  # Remove leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('combined_files.csv')\n",
        "\n",
        "# Apply preprocessing to the 'Content' column\n",
        "df['clean_content'] = df['Content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "xNJk4rDivfAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Extractive Summary"
      ],
      "metadata": {
        "id": "Yq8xImryIXcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Extractive Summarisation using TF-IDF\n",
        "extractive summarisation using the TF-IDF (Term Frequency-Inverse Document Frequency) method:\n",
        "\n",
        "1. **Calculate TF-IDF scores for each term in the document**.\n",
        "2. **Rank sentences based on the sum of TF-IDF scores of their terms**.\n",
        "3. **Select the top N sentences with the highest scores as the summary**."
      ],
      "metadata": {
        "id": "sJi1e361GBfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "# Function to generate extractive summary using TF-IDF\n",
        "def extractive_summary(text, top_n=3):\n",
        "    sentences = text.split('.')\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sentence_scores = np.sum(X.toarray(), axis=1)\n",
        "    top_sentence_indices = sentence_scores.argsort()[-top_n:][::-1]\n",
        "    summary = '. '.join([sentences[i].strip() for i in top_sentence_indices if sentences[i].strip()]) + '.'\n",
        "    return summary\n",
        "\n",
        "# Apply the summarisation function to each content\n",
        "df['summary_tfidf'] = df['clean_content'].apply(extractive_summary)\n",
        "\n",
        "# Display the DataFrame with summaries\n",
        "print(df[['Title', 'clean_content','summary_tfidf']])"
      ],
      "metadata": {
        "id": "J8UjBBFQHv_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk6BTPC40htn"
      },
      "source": [
        "## Extractive Summarisation using Sumy and Summa\n",
        "Implement a simple text summariser based on sentence extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "!pip install nltk\n",
        "!pip install summa\n",
        "!pip install seaborn mglearn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from summa import summarizer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "18J-zQbx5nTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### summarising Text Using LSA summariser\n",
        "\n",
        "utilize the Latent Semantic Analysis (LSA) summariser from the Sumy package to summarise text. The LSA summariser works by reducing the dimensionality of the text representation and selecting the most significant sentences to form a coherent summary.\n",
        "\n",
        "1. **Define the summarisation Function**:\n",
        "   - We create a function `summarise_text` that accepts the text to be summarised and the number of sentences desired in the summary.\n",
        "   - This function utilizes the LSA summariser to generate the summary.\n",
        "\n",
        "2. **Apply the summarisation Function**:\n",
        "   - We apply the `summarise_text` function to the 'clean_content' column of our DataFrame.\n",
        "   - The resulting summaries are stored in a new column called 'sumy_summary'.\n",
        "\n",
        "3. **Display and Save the summarised Data**:\n",
        "   - We display the DataFrame containing the new summaries.\n",
        "   - Finally, we save the DataFrame with the summaries to a CSV file for further use or analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "PeovpKTehQqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarise_text(text, sentences_count=2):\n",
        "    \"\"\"summarise the text using LSA summariser\"\"\"\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summariser = LsaSummarizer()\n",
        "    summary = summariser(parser.document, sentences_count)\n",
        "    return ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "# Apply summarisation to the 'clean_content' column\n",
        "df['summary_sumy'] = df['clean_content'].apply(lambda x: summarise_text(x, sentences_count=2))\n",
        "\n",
        "# Display the DataFrame with summaries\n",
        "print(df[['Title', 'summary_sumy']])\n",
        "\n",
        "# Save the DataFrame with summaries to a CSV file\n",
        "df.to_csv('sumy_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "Jy_DgBWN6v69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### summarising Text Using the Summa Library\n",
        "\n",
        "utilize the `summa` library to summarise text. The `summa` library leverages the TextRank algorithm to generate concise and effective summaries.\n",
        "\n",
        "1. **Define the summarisation Function**:\n",
        "   - We create a function `summa_summarise` that accepts the text to be summarised and a `ratio` parameter. The `ratio` determines the fraction of the original text to be retained in the summary.\n",
        "   - This function uses the `summarizer.summarize` method from the `summa` library to perform the summarization.\n",
        "\n",
        "2. **Apply the summarisation Function**:\n",
        "   - We apply the `summa_summarize` function to the 'clean_content' column of our DataFrame.\n",
        "   - The resulting summaries are stored in a new column called 'summa_summary'.\n",
        "\n",
        "3. **Save the summarised Data**:\n",
        "   - Finally, we save the DataFrame with the new summaries to a CSV file for further use or analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "pq88tmYmhrNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summa_summarise(text, ratio=0.2):\n",
        "    \"\"\"summarise text using the summa library\"\"\"\n",
        "    summary = summarizer.summarize(text, ratio=ratio, language='english')\n",
        "    return summary\n",
        "# Apply summarisation to the 'clean_content' column using summa\n",
        "df['summary_summa'] = df['clean_content'].apply(lambda x: summa_summarise(x, ratio=0.2))\n",
        "# Save the cleaned and summarised data to a CSV file if needed\n",
        "df.to_csv('summa_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "F9aem_NeJQ9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efn9Q1mF0hto"
      },
      "source": [
        "### Save summarised Data\n",
        "Save the summarised data to a new CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN4ik2T10hto"
      },
      "outputs": [],
      "source": [
        "output_file = 'summarized_data.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Summarized data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6VnEXxC0hto"
      },
      "source": [
        "##  Visualise Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        ">To visualise the most frequent words in the summaries from each method using **word cloud**:\n",
        "---"
      ],
      "metadata": {
        "id": "yWPPhw4qiaSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "nzjdisRUD_3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "summarisation_methods = ['summary_sumy', 'summary_summa', 'summary_tfidf']\n",
        "# Generate word clouds for each summarisation method\n",
        "for method in summarisation_methods:\n",
        "    text = ' '.join(df[method])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for {method}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "W289Icx4iaSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        ">Visualise the length of original texts vs. summaries.\n",
        "---"
      ],
      "metadata": {
        "id": "NOlptMC_ig69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "f4MsAeX2YQ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbDf32ZK0hto"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('summarized_data.csv')\n",
        "\n",
        "# Calculating the lengths of the original and summarized texts\n",
        "df['original_length'] = df['clean_content'].apply(lambda x: len(x.split()))\n",
        "df['summary_sumy_length'] = df['summary_sumy'].apply(lambda x: len(x.split()))\n",
        "df['summary_summa_length'] = df['summary_summa'].apply(lambda x: len(x.split()))\n",
        "df['summary_tfidf_length'] = df['summary_tfidf'].apply(lambda x: len(x.split()))\n",
        "#df['summary_transformer_length'] = df['summary_transformer'].apply(lambda x: len(x.split()))  # Assuming transformer summary column exists\n",
        "\n",
        "# Plotting the distribution of text lengths\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.histplot(df['original_length'], kde=True, color='blue', label='Original Length', bins=30)\n",
        "sns.histplot(df['summary_sumy_length'], kde=True, color='red', label='Sumy Summary Length', bins=30)\n",
        "sns.histplot(df['summary_summa_length'], kde=True, color='green', label='Summa Summary Length', bins=30)\n",
        "sns.histplot(df['summary_tfidf_length'], kde=True, color='purple', label='TFIDF Summary Length', bins=30)\n",
        "#sns.histplot(df['summary_transformer_length'], kde=True, color='orange', label='Transformer Summary Length', bins=30)\n",
        "plt.legend()\n",
        "plt.title('Distribution of Text Lengths for Different summarisation Methods')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Do you think of better visualisation?**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UZtwDqAiA-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Box Plots\n",
        "**Box Plots**:\n",
        "   Box plots can show the distribution, median, and outliers for each summarisation method in a more compact way."
      ],
      "metadata": {
        "id": "zemIGLXKTSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "sns.boxplot(data=df[['original_length', 'summary_sumy_length', 'summary_summa_length', 'summary_tfidf_length']])\n",
        "plt.title('Box Plot of Text Lengths for Different summarisation Methods')\n",
        "plt.xlabel('Text Type')\n",
        "plt.ylabel('Length of Text')\n",
        "plt.xticks([0, 1, 2, 3, 4], ['Original', 'Sumy', 'Summa', 'TFIDF', 'Transformer'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BjA6zy-nSzho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bar Plots of Mean Lengths\n",
        "**Bar Plots of Mean Lengths**:\n",
        "   A bar plot can provide a quick comparison of the average lengths of the original texts and their summaries.\n"
      ],
      "metadata": {
        "id": "T9ZPeu7BTpiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_lengths = {\n",
        "    'Original': df['original_length'].mean(),\n",
        "    'Sumy': df['summary_sumy_length'].mean(),\n",
        "    'Summa': df['summary_summa_length'].mean(),\n",
        "    'TFIDF': df['summary_tfidf_length'].mean()\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(mean_lengths.keys(), mean_lengths.values(), color=['blue', 'red', 'green', 'purple', 'orange'])\n",
        "plt.title('Average Text Lengths for Different Summarization Methods')\n",
        "plt.xlabel('Text Type')\n",
        "plt.ylabel('Average Length of Text')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OwUHVEJITGZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###3. Pairwise Comparison with Scatter Plots\n",
        "**Pairwise Comparison with Scatter Plots**:\n",
        "   Scatter plots can help visualize the relationship between the lengths of original texts and their summaries for each method.\n"
      ],
      "metadata": {
        "id": "aQBiBYERTsj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(df['original_length'], df['summary_sumy_length'], color='red', label='Sumy', alpha=0.6)\n",
        "plt.scatter(df['original_length'], df['summary_summa_length'], color='green', label='Summa', alpha=0.6)\n",
        "plt.scatter(df['original_length'], df['summary_tfidf_length'], color='purple', label='TFIDF', alpha=0.6)\n",
        "plt.title('Pairwise Comparison of Text Lengths')\n",
        "plt.xlabel('Original Length of Text')\n",
        "plt.ylabel('Summary Length of Text')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qLtrM5oNTr_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Distribution Plot with KDE\n",
        "**Distribution Plot with KDE**:\n",
        "   A more detailed distribution plot using Kernel Density Estimation (KDE) to show the continuous distribution of text lengths."
      ],
      "metadata": {
        "id": "WaBzN-61TutO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "sns.kdeplot(df['original_length'], color='blue', label='Original Length', fill=True)\n",
        "sns.kdeplot(df['summary_sumy_length'], color='red', label='Sumy Summary Length', fill=True)\n",
        "sns.kdeplot(df['summary_summa_length'], color='green', label='Summa Summary Length', fill=True)\n",
        "sns.kdeplot(df['summary_tfidf_length'], color='purple', label='TFIDF Summary Length', fill=True)\n",
        "\n",
        "plt.title('KDE Plot of Text Lengths for Different summarisation Methods')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4uY3TZbRTvfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Violin Plots\n",
        "Violin plots combine aspects of box plots and KDE plots, providing a richer visualization of the data distribution.\n"
      ],
      "metadata": {
        "id": "MsZx8H8ZUfB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.violinplot(data=df[['original_length', 'summary_sumy_length', 'summary_summa_length', 'summary_tfidf_length']])\n",
        "plt.title('Violin Plot of Text Lengths for Different summarisation Methods')\n",
        "plt.xlabel('Text Type')\n",
        "plt.ylabel('Length of Text')\n",
        "plt.xticks([0, 1, 2, 3, 4], ['Original', 'Sumy', 'Summa', 'TFIDF', 'Transformer'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BgXa7SM9UdT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Heatmap of Correlations\n",
        "A heatmap can show the correlation between the lengths of texts from different summarisation methods.\n"
      ],
      "metadata": {
        "id": "y05mVQl4Unbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df[['original_length', 'summary_sumy_length', 'summary_summa_length', 'summary_tfidf_length']].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True)\n",
        "plt.title('Correlation Heatmap of Text Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KPAD4t5eUqhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Paired Density and Scatter Plot (Pairplot)\n",
        "Pairplots can show pairwise relationships and distributions of text lengths for different methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "AUcQsscRU8S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.pairplot(df[['original_length', 'summary_sumy_length', 'summary_summa_length', 'summary_tfidf_length']], kind='scatter', diag_kind='kde')\n",
        "plt.suptitle('Pairplot of Text Lengths for Different summarisation Methods', y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PrTT3CszU5DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 8. Joint Plot\n",
        "Joint plots provide a way to visually represent the relationship between two variables along with their distributions.\n"
      ],
      "metadata": {
        "id": "WYA-wVDuVPtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "sns.jointplot(x='original_length', y='summary_sumy_length', data=df, kind='scatter', color='red', marginal_kws=dict(bins=30, fill=True))\n",
        "plt.suptitle('Joint Plot of Original vs Sumy Summary Length', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-2ZRlonvVOwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. 3D Scatter Plot\n",
        "visualize three dimensions of the data (e.g., lengths of original, Sumy, and Summa summaries), a 3D scatter plot.\n"
      ],
      "metadata": {
        "id": "EqymY1tPVq6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(df['original_length'], df['summary_sumy_length'], df['summary_summa_length'], c='r', marker='o')\n",
        "ax.set_xlabel('Original Length')\n",
        "ax.set_ylabel('Sumy Summary Length')\n",
        "ax.set_zlabel('Summa Summary Length')\n",
        "plt.title('3D Scatter Plot of Text Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIF20qneVkXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Abstarctive summary\n",
        "Creating a summariser involves several steps, especially for abstractive summarisation:\n",
        "\n",
        "1. **Text Preprocessing**: Clean and prepare the text by removing stop words, punctuation, and performing tokenization.\n",
        "2. **Feature Extraction**: For extractive summarisation, extract features like sentence importance, frequency of terms, etc. For abstractive summarisation, use techniques like word embeddings.\n",
        "3. **Model Training**: Train a model to understand the structure and content of the text. This could be a simple model for extractive summarisation or a complex neural network for abstractive summarisation.\n",
        "4. **Generation**: Generate the summary by selecting key sentences (extractive) or generating new sentences (abstractive).\n",
        "\n"
      ],
      "metadata": {
        "id": "HGU-wHQcIK-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step to install the transformers library\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "zYl7SDQq0lJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a summarisation pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    \"\"\"Use the summarizer to condense the text\"\"\"\n",
        "    try:\n",
        "        summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing text: {e}\")\n",
        "        return text  # Return original text if summarization fails\n",
        "\n",
        "# Apply summarization to the 'clean_content' column\n",
        "df['summary_transformer'] = df['clean_content'].apply(summarize_text)\n",
        "\n",
        "print(\"\\nFirst few rows of the summarized dataset:\")\n",
        "print(df[['clean_content', 'summary_transformer']].head())\n"
      ],
      "metadata": {
        "id": "L70XjLSYLi1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "url = 'https://drive.google.com/uc?id=1J0U-Ho_qH73wtjRYjDSKzaDT274zdljV'\n",
        "output = 'Goldstandard.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "\n",
        "if zipfile.is_zipfile(output):\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir('/content/'))\n",
        "else:\n",
        "    print(\"Downloaded file is not a zip file.\")"
      ],
      "metadata": {
        "id": "lFS36i0icA3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining Source Texts and Gold Standard Summaries\n",
        "\n",
        "In this section, we will combine source texts and their corresponding gold standard summaries into a single DataFrame, perform basic preprocessing on the text content, and prepare the data for further analysis.\n",
        "\n",
        "1. **Directory Paths**:\n",
        "   - Define paths for source texts (`source_dir`) and gold standard summaries (`gold_summary_dir`).\n",
        "\n",
        "2. **Combine Files into a DataFrame**:\n",
        "   - `combine_files_to_dataframe(directory_path)`: Reads files from the specified directory and combines their contents into a DataFrame.\n",
        "\n",
        "3. **Load Source Texts and Gold Standard Summaries**:\n",
        "   - Load source texts into `df_source` and remove the '.english' suffix from titles.\n",
        "   - Load gold standard summaries into `df_gold`.\n",
        "\n",
        "4. **Merge DataFrames**:\n",
        "   - Merge `df_source` and `df_gold` on 'Title', creating a combined DataFrame with suffixes `('_source', '_gold')`.\n",
        "\n",
        "5. **Select a Subset of Data**:\n",
        "   - Select the first 10 rows for analysis.\n",
        "\n",
        "6. **Preprocess Text**:\n",
        "   - `clean_text(text)`: Cleans text by removing HTML tags, extra spaces, standalone numbers, and leading/trailing whitespace.\n",
        "\n",
        "7. **Apply Preprocessing**:\n",
        "   - Apply `clean_text` to the 'Content_source' column and store the result in 'clean_content'.\n",
        "\n"
      ],
      "metadata": {
        "id": "zr3uGQL7k43r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Directory paths\n",
        "source_dir = '/content/SourceTexts/english'\n",
        "gold_summary_dir = '/content/Goldstandard'\n",
        "\n",
        "\n",
        "# Function to combine files into a DataFrame\n",
        "def combine_files_to_dataframe(directory_path):\n",
        "    data = []\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                data.append({'Title': file_name, 'Content': content.strip()})\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Load source texts\n",
        "df_source = combine_files_to_dataframe(source_dir)\n",
        "df_source['Title'] = df_source['Title'].str.replace('.english', '', regex=False)\n",
        "\n",
        "# Load gold standard summaries\n",
        "df_gold = combine_files_to_dataframe(gold_summary_dir)\n",
        "\n",
        "# Merge dataframes on Title\n",
        "df = pd.merge(df_source, df_gold, on='Title', suffixes=('_source', '_gold'))\n",
        "\n",
        "# Select the first 10 files\n",
        "df = df.head(10)\n",
        "\n",
        "# Preprocess text function (clean HTML tags, remove stop words, etc.)\n",
        "def clean_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to the 'Content_source' column\n",
        "df['clean_content'] = df['Content_source'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "PPZOM3VR2xM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Define summarisation Functions\n",
        "\n",
        "1. **summarise Text Using Sumy (LSA)**:\n",
        "   - `summarize_text_sumy(text, sentences_count=2)`: Uses the LSA summarizer from the `sumy` package to generate a summary with a specified number of sentences.\n",
        "\n",
        "2. **Generate Extractive Summary Using TF-IDF**:\n",
        "   - `extractive_summary(text, top_n=3)`: Uses TF-IDF to score sentences based on term frequency-inverse document frequency and selects the top N sentences for the summary.\n",
        "\n",
        "3. **summarise Text Using Summa (TextRank)**:\n",
        "   - `summarize_text_summa(text)`: Uses the `summa` library to generate a summary based on the TextRank algorithm.\n",
        "\n",
        "4. **summarise Text Using Transformer**:\n",
        "   - Load the summarization pipeline: `transformer_summarizer = pipeline(\"summarization\")`.\n",
        "   - `summarize_text_transformer(text)`: Uses a pre-trained transformer model to generate a summary, with constraints on the maximum and minimum length of the summary.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GvRXt5SflP3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from summa import summarizer as summa_summarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "# Function to summarize text using Sumy\n",
        "def summarize_text_sumy(text, sentences_count=2):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary = summarizer(parser.document, sentences_count)\n",
        "    return ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "# Function to generate extractive summary using TF-IDF\n",
        "def extractive_summary(text, top_n=3):\n",
        "    sentences = text.split('.')\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(sentences)\n",
        "    sentence_scores = np.sum(X.toarray(), axis=1)\n",
        "    top_sentence_indices = sentence_scores.argsort()[-top_n:][::-1]\n",
        "    summary = ' '.join([sentences[i] for i in top_sentence_indices])\n",
        "    return summary\n",
        "\n",
        "# Function to summarize text using Summa\n",
        "def summarize_text_summa(text):\n",
        "    summary = summa_summarizer.summarize(text)\n",
        "    return summary\n",
        "\n",
        "# Load the summarization pipeline for Transformer\n",
        "transformer_summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Function to summarize text using Transformer\n",
        "def summarize_text_transformer(text):\n",
        "    try:\n",
        "        summary = transformer_summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing text: {e}\")\n",
        "        return text\n",
        "\n",
        "# Generate summaries\n",
        "df['summary_sumy'] = df['clean_content'].apply(lambda x: summarize_text_sumy(x, sentences_count=2))\n",
        "df['summary_tfidf'] = df['clean_content'].apply(extractive_summary)\n",
        "df['summary_summa'] = df['clean_content'].apply(summarize_text_summa)\n",
        "df['summary_transformer'] = df['clean_content'].apply(summarize_text_transformer)\n"
      ],
      "metadata": {
        "id": "uPxTR6jk4MpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Now we can visualise the summaries in a table using Plotly, allowing us to view the content summarisation results side by side for easy comparison.**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iaf_vIYGluye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import plotly.graph_objects as go\n",
        "# Visualize the data using Plotly\n",
        "fig = go.Figure(data=[go.Table(\n",
        "    header=dict(values=df.columns[:8],\n",
        "                fill_color='paleturquoise',\n",
        "align='left'),\n",
        "    cells=dict(values=[df['Title'], df['Content_source'], df['Content_gold'],df['clean_content'],df['summary_sumy'],df['summary_tfidf'],df['summary_summa'], df['summary_transformer']],\n",
        "               fill_color='lavender',\n",
        "               align='left'))\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Content Summarization',\n",
        "    width=1000,\n",
        "    height=600,\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "joGvWAeIJsnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **we can also displays sample summaries from each method, allowing for a quick comparison of original content and generated summaries.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1WkswtBbmHHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sample summaries from each method\n",
        "sample_indices = [0, 1, 2]  # Adjust these indices to sample different rows\n",
        "\n",
        "for i in sample_indices:\n",
        "    print(f\"Original Content [{i}]:\\n{df['clean_content'][i]}\\n\")\n",
        "    print(f\"Sumy Summary [{i}]:\\n{df['summary_sumy'][i]}\\n\")\n",
        "    print(f\"Summa Summary [{i}]:\\n{df['summary_summa'][i]}\\n\")\n",
        "    print(f\"TFIDF Summary [{i}]:\\n{df['summary_tfidf'][i]}\\n\")\n",
        "    print(f\"Transformer Summary [{i}]:\\n{df['summary_transformer'][i]}\\n\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "hlymPPxXW0y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following sections analyse and visualize different aspects of text summarisation methods. We will conduct four main analyses:\n",
        "\n",
        "1. **Trend Analysis of Summary Lengths:** Examine the average lengths of summaries generated by different methods.\n",
        "2. **Word Cloud Visualization:** Visualize the most frequent words in the summaries.\n",
        "3. **Readability Scores:** Calculate and compare the readability of summaries.\n",
        "4. **Semantic Similarity:** Measure the semantic similarity between the original text and the summaries.\n"
      ],
      "metadata": {
        "id": "5TKY-hyKm9KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Trend Analysis of Summary Lengths\n",
        "To compare the average length of the original texts with the lengths of summaries produced by various summarisation methods. This helps in understanding how concise each method is in summarising the content."
      ],
      "metadata": {
        "id": "TwdkMW_Cdkq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('summarised_data.csv')\n",
        "\n",
        "# Calculating the lengths of the original and summarised texts\n",
        "df['original_length'] = df['clean_content'].apply(lambda x: len(x.split()))\n",
        "df['summary_sumy_length'] = df['summary_sumy'].apply(lambda x: len(x.split()))\n",
        "df['summary_summa_length'] = df['summary_summa'].apply(lambda x: len(x.split()))\n",
        "df['summary_tfidf_length'] = df['summary_tfidf'].apply(lambda x: len(x.split()))\n",
        "df['summary_transformer_length'] = df['summary_transformer'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Calculate average lengths\n",
        "average_lengths = {\n",
        "    'Original': df['original_length'].mean(),\n",
        "    'Sumy': df['summary_sumy_length'].mean(),\n",
        "    'Summa': df['summary_summa_length'].mean(),\n",
        "    'TFIDF': df['summary_tfidf_length'].mean(),\n",
        "    'Transformer': df['summary_transformer_length'].mean()\n",
        "}\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "average_lengths_df = pd.DataFrame(list(average_lengths.items()), columns=['Method', 'Average Length'])\n",
        "\n",
        "# Plotting the trend\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.lineplot(data=average_lengths_df, x='Method', y='Average Length', marker='o')\n",
        "plt.title('Trend of Average Text Lengths Across summarisation Methods')\n",
        "plt.xlabel('summarisation Method')\n",
        "plt.ylabel('Average Length of Text')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "W_fXLMVAWt0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Word Cloud\n",
        "\n",
        "To visualize the most frequent words in the summaries generated by different methods, providing insight into the key terms emphasized by each summarisation technique."
      ],
      "metadata": {
        "id": "fvy_N7vAX0fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "summarisation_methods = ['summary_sumy', 'summary_summa', 'summary_tfidf']\n",
        "# Generate word clouds for each summarisation method\n",
        "for method in summarisation_methods:\n",
        "    text = ' '.join(df[method])\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Word Cloud for {method}')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mkGl4WoUXHqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Readability Scores\n",
        "To calculate and compare the readability of summaries generated by different methods. Readability scores indicate how easy it is to read and understand the text."
      ],
      "metadata": {
        "id": "-Pvti4bdX4B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nstall and Import textstat:\n",
        "Install the `textstat` package for calculating readability scores.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ejz-RTmwn8Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "ccQGKWDsX_Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textstat import textstat\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pprint\n",
        "# Function to calculate readability scores\n",
        "def calculate_readability_scores(df, summarisation_methods):\n",
        "    readability_scores = {}\n",
        "    for method in summarisation_methods:\n",
        "        method_scores = []\n",
        "        for summary in df[method]:\n",
        "            score = textstat.flesch_reading_ease(summary)\n",
        "            method_scores.append(score)\n",
        "        avg_score = sum(method_scores) / len(method_scores)\n",
        "        readability_scores[method] = avg_score\n",
        "    return readability_scores\n",
        "\n",
        "readability_scores = calculate_readability_scores(df, summarisation_methods)\n",
        "\n",
        "# Print readability scores\n",
        "pprint.pprint(readability_scores)\n"
      ],
      "metadata": {
        "id": "85Y1UW-nX3mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Semantic Similarity\n",
        "\n",
        "To measure the cosine similarity between the original text and the summaries, providing insight into how well the summaries capture the meaning of the original content."
      ],
      "metadata": {
        "id": "vwQ8vDUIXrJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pprint\n",
        "# Function to calculate semantic similarity\n",
        "def calculate_semantic_similarity(df, summarisation_methods):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    similarities = {}\n",
        "    for method in summarisation_methods:\n",
        "        method_similarities = []\n",
        "        for i, row in df.iterrows():\n",
        "            try:\n",
        "                vectors = vectorizer.fit_transform([row['clean_content'], row[method]])\n",
        "                sim = cosine_similarity(vectors[0:1], vectors[1:2])\n",
        "                method_similarities.append(sim[0][0])\n",
        "            except:\n",
        "                pass\n",
        "        avg_similarity = sum(method_similarities) / len(method_similarities)\n",
        "        similarities[method] = avg_similarity\n",
        "    return similarities\n",
        "\n",
        "semantic_similarities = calculate_semantic_similarity(df, summarisation_methods)\n",
        "\n",
        "# Print semantic similarity scores\n",
        "pprint.pprint(semantic_similarities)\n"
      ],
      "metadata": {
        "id": "33bman6bXW2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5-Evaluation**:\n",
        "\n",
        "### 1. **BLEU (Bilingual Evaluation Understudy)**\n",
        "- Originally designed for machine translation but can be used for summarisation.\n",
        "- Measures the precision of n-grams in the generated summary against the reference summary.\n",
        "\n",
        "### 2. **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**\n",
        "- Another metric from the machine translation community.\n",
        "- Considers synonymy and stemming, providing a more nuanced evaluation than BLEU.\n",
        "\n",
        "### 3. **CIDEr (Consensus-based Image Description Evaluation)**\n",
        "- Used mainly in image captioning, but applicable to summarisation.\n",
        "- Measures the consensus between generated and reference summaries using Term Frequency-Inverse Document Frequency (TF-IDF) weighting.\n",
        "\n",
        "### 4. **BERTScore**\n",
        "- Leverages pre-trained BERT embeddings to evaluate summaries.\n",
        "- Measures the similarity between the embeddings of the generated and reference summaries.\n",
        "\n",
        "### 5. **Human Evaluation**\n",
        "- **Fluency**: Is the summary grammatically correct and well-structured?\n",
        "- **Relevance**: Does the summary include the most important points of the original text?\n",
        "- **Coherence**: Is the summary logically organized?\n",
        "- **Informativeness**: Does the summary provide enough information?\n",
        "\n",
        "### 6. **summarisation-Specific Metrics**\n",
        "- **Pyramid Method**: Involves human annotators identifying and scoring key content units in the summary and reference texts.\n",
        "- **Content Responsiveness**: Evaluates how well the summary responds to specific questions related to the source text.\n"
      ],
      "metadata": {
        "id": "LAf2fYqUnWwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 ROUGE Scores**\n",
        "To evaluate the quality of summaries using ROUGE scores, which compare the overlap of n-grams between the generated summaries and reference texts."
      ],
      "metadata": {
        "id": "AaR5jWNCoVjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install and Import rouge-score:\n",
        "\n",
        "Install the `rouge-score` package for calculating ROUGE scores."
      ],
      "metadata": {
        "id": "Geqr0Dh2ojBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score  # Install the rouge-score package"
      ],
      "metadata": {
        "id": "_xDATliQ6MQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate ROUGE Scores:\n",
        "\n",
        "Define a function to compute ROUGE scores and apply it to the summaries."
      ],
      "metadata": {
        "id": "xqqUQBLco1KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "# Function to calculate ROUGE scores\n",
        "def calculate_rouge_scores(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores\n",
        "\n",
        "# Evaluate summaries\n",
        "df['rouge_sumy'] = df.apply(lambda row: calculate_rouge_scores(row['Content_gold'], row['summary_sumy']), axis=1)\n",
        "df['rouge_tfidf'] = df.apply(lambda row: calculate_rouge_scores(row['Content_gold'], row['summary_tfidf']), axis=1)\n",
        "df['rouge_summa'] = df.apply(lambda row: calculate_rouge_scores(row['Content_gold'], row['summary_summa']), axis=1)\n",
        "df['rouge_transformer'] = df.apply(lambda row: calculate_rouge_scores(row['Content_gold'], row['summary_transformer']), axis=1)\n"
      ],
      "metadata": {
        "id": "-fgDwB5B4Qwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Extract and Plot ROUGE Scores:\n",
        "Extract F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L, and plot them."
      ],
      "metadata": {
        "id": "qW6OpVaVo9-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract ROUGE-1 F1 scores\n",
        "df['rouge_sumy_1'] = df['rouge_sumy'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "df['rouge_tfidf_1'] = df['rouge_tfidf'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "df['rouge_summa_1'] = df['rouge_summa'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "df['rouge_transformer_1'] = df['rouge_transformer'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "\n",
        "# Extract ROUGE-2 F1 scores\n",
        "df['rouge_sumy_2'] = df['rouge_sumy'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "df['rouge_tfidf_2'] = df['rouge_tfidf'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "df['rouge_summa_2'] = df['rouge_summa'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "df['rouge_transformer_2'] = df['rouge_transformer'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "\n",
        "# Plot ROUGE-1 scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['Title'], df['rouge_sumy_1'], label='Sumy ROUGE-1', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_tfidf_1'], label='TF-IDF ROUGE-1', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_summa_1'], label='Summa ROUGE-1', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_transformer_1'], label='Transformer ROUGE-1', marker='o')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('ROUGE-1 F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('ROUGE-1 F1 Scores for Different summarisation Methods')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROUGE-2 scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['Title'], df['rouge_sumy_2'], label='Sumy ROUGE-2', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_tfidf_2'], label='TF-IDF ROUGE-2', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_summa_2'], label='Summa ROUGE-2', marker='o')\n",
        "plt.plot(df['Title'], df['rouge_transformer_2'], label='Transformer ROUGE-2', marker='o')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('ROUGE-2 F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('ROUGE-2 F1 Scores for Different summarisation Methods')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fJMqhge959Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract ROUGE-1, ROUGE-2, and ROUGE-L F1 scores\n",
        "df['rouge_sumy_1'] = df['rouge_sumy'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "df['rouge_tfidf_1'] = df['rouge_tfidf'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "df['rouge_transformer_1'] = df['rouge_transformer'].apply(lambda x: x['rouge1'].fmeasure)\n",
        "\n",
        "df['rouge_sumy_2'] = df['rouge_sumy'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "df['rouge_tfidf_2'] = df['rouge_tfidf'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "df['rouge_transformer_2'] = df['rouge_transformer'].apply(lambda x: x['rouge2'].fmeasure)\n",
        "\n",
        "df['rouge_sumy_L'] = df['rouge_sumy'].apply(lambda x: x['rougeL'].fmeasure)\n",
        "df['rouge_tfidf_L'] = df['rouge_tfidf'].apply(lambda x: x['rougeL'].fmeasure)\n",
        "df['rouge_transformer_L'] = df['rouge_transformer'].apply(lambda x: x['rougeL'].fmeasure)\n",
        "\n",
        "# Plot ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
        "\n",
        "# Bar Chart of Average ROUGE Scores\n",
        "average_rouge_scores = {\n",
        "    'Sumy ROUGE-1': df['rouge_sumy_1'].mean(),\n",
        "    'TF-IDF ROUGE-1': df['rouge_tfidf_1'].mean(),\n",
        "    'Transformer ROUGE-1': df['rouge_transformer_1'].mean(),\n",
        "    'Sumy ROUGE-2': df['rouge_sumy_2'].mean(),\n",
        "    'TF-IDF ROUGE-2': df['rouge_tfidf_2'].mean(),\n",
        "    'Transformer ROUGE-2': df['rouge_transformer_2'].mean(),\n",
        "    'Sumy ROUGE-L': df['rouge_sumy_L'].mean(),\n",
        "    'TF-IDF ROUGE-L': df['rouge_tfidf_L'].mean(),\n",
        "    'Transformer ROUGE-L': df['rouge_transformer_L'].mean()\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for easy plotting\n",
        "avg_rouge_df = pd.DataFrame(list(average_rouge_scores.items()), columns=['Metric', 'Average Score'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(avg_rouge_df['Metric'], avg_rouge_df['Average Score'], color=['blue', 'green', 'red', 'blue', 'green', 'red', 'blue', 'green', 'red'])\n",
        "plt.ylabel('Average ROUGE Score')\n",
        "plt.title('Average ROUGE Scores for Different summarisation Methods')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Scatter Plots of Individual Document Scores\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(df['Title'], df['rouge_sumy_1'], label='Sumy ROUGE-1', marker='o')\n",
        "plt.scatter(df['Title'], df['rouge_tfidf_1'], label='TF-IDF ROUGE-1', marker='x')\n",
        "plt.scatter(df['Title'], df['rouge_transformer_1'], label='Transformer ROUGE-1', marker='^')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('ROUGE-1 F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('ROUGE-1 F1 Scores for Different summarisation Methods')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(df['Title'], df['rouge_sumy_2'], label='Sumy ROUGE-2', marker='o')\n",
        "plt.scatter(df['Title'], df['rouge_tfidf_2'], label='TF-IDF ROUGE-2', marker='x')\n",
        "plt.scatter(df['Title'], df['rouge_transformer_2'], label='Transformer ROUGE-2', marker='^')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('ROUGE-2 F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('ROUGE-2 F1 Scores for Different summarisation Methods')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(df['Title'], df['rouge_sumy_L'], label='Sumy ROUGE-L', marker='o')\n",
        "plt.scatter(df['Title'], df['rouge_tfidf_L'], label='TF-IDF ROUGE-L', marker='x')\n",
        "plt.scatter(df['Title'], df['rouge_transformer_L'], label='Transformer ROUGE-L', marker='^')\n",
        "plt.xlabel('Document')\n",
        "plt.ylabel('ROUGE-L F1 Score')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('ROUGE-L F1 Scores for Different summarisation Methods')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p2ai2lZl8I4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Sumy:**\n",
        "  - ROUGE-1: ~0.12\n",
        "  - ROUGE-2: ~0.025\n",
        "  - ROUGE-L: ~0.08\n",
        "- **TF-IDF:**\n",
        "  - ROUGE-1: ~0.20\n",
        "  - ROUGE-2: ~0.04\n",
        "  - ROUGE-L: ~0.11\n",
        "- **Transformer:**\n",
        "  - ROUGE-1: ~0.10\n",
        "  - ROUGE-2: ~0.025\n",
        "  - ROUGE-L: ~0.07\n",
        "\n",
        "### Analysis\n",
        "1. **ROUGE-1:**\n",
        "   - **TF-IDF** has the highest average ROUGE-1 score (~0.20), indicating it captures the most unigram (single word) matches compared to the other methods.\n",
        "\n",
        "2. **ROUGE-2:**\n",
        "   - **TF-IDF** again has the highest average ROUGE-2 score (~0.04), suggesting it captures the most bigram (two consecutive words) matches.\n",
        "\n",
        "3. **ROUGE-L:**\n",
        "   - **TF-IDF** has the highest average ROUGE-L score (~0.11), which measures the longest common subsequence and indicates it captures the structure of the original text better.\n",
        "\n",
        "Based on the average ROUGE scores, the **TF-IDF summarizer** appears to be the best performer among the three methods (Sumy, TF-IDF, and Transformer). It consistently has the highest scores across ROUGE-1, ROUGE-2, and ROUGE-L metrics.\n"
      ],
      "metadata": {
        "id": "_KRTRIFw8xeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.1 Bert Score**\n",
        "\n",
        "evaluate the quality of generated summaries using BERTScore, a state-of-the-art metric that leverages BERT embeddings to measure the similarity between the reference and generated texts. BERTScore provides a more nuanced understanding of text similarity compared to traditional metrics."
      ],
      "metadata": {
        "id": "byN98C0ZpJ2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "1MixNPju9RB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bert_score import score\n",
        "\n",
        "def calculate_bertscore(reference_texts, generated_texts):\n",
        "    P, R, F1 = score(generated_texts, reference_texts, lang=\"en\", verbose=True)\n",
        "    return P.mean().item(), R.mean().item(), F1.mean().item()\n",
        "\n",
        "# Extract reference and generated summaries\n",
        "reference_texts = df['Content_gold'].tolist()\n",
        "generated_sumy = df['summary_sumy'].tolist()\n",
        "generated_tfidf = df['summary_tfidf'].tolist()\n",
        "generated_transformer = df['summary_transformer'].tolist()\n",
        "\n",
        "# Calculate BERTScores\n",
        "bertscore_sumy = calculate_bertscore(reference_texts, generated_sumy)\n",
        "bertscore_tfidf = calculate_bertscore(reference_texts, generated_tfidf)\n",
        "bertscore_transformer = calculate_bertscore(reference_texts, generated_transformer)\n",
        "\n",
        "print(f\"BERTScore (Sumy): Precision={bertscore_sumy[0]}, Recall={bertscore_sumy[1]}, F1={bertscore_sumy[2]}\")\n",
        "print(f\"BERTScore (TF-IDF): Precision={bertscore_tfidf[0]}, Recall={bertscore_tfidf[1]}, F1={bertscore_tfidf[2]}\")\n",
        "print(f\"BERTScore (Transformer): Precision={bertscore_transformer[0]}, Recall={bertscore_transformer[1]}, F1={bertscore_transformer[2]}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uF54gG4j8_HB",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
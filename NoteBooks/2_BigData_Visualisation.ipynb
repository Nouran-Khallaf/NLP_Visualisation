{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCREL/Session1_Visualisation_and_Summarisation/blob/main/NoteBooks/2_BigData_Visualisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-document Summaries Corpora **\n",
        "The dataset is derived from publicly available WikiNews (http://www.wikinews.org/) English texts.\n",
        "\n",
        "The source texts were under CC Attribution Licence V2.5 (cf. http://creativecommons.org/licenses/by/2.5/). Texts in other languages have been translated by native speakers of each language.\n",
        "\n",
        "https://www.lancaster.ac.uk/staff/elhaj/corpora.html"
      ],
      "metadata": {
        "id": "EGOlM0g_1dqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install plotly\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install tabulate\n",
        "!pip install summa"
      ],
      "metadata": {
        "id": "Vju1fXIT1qXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load and Prepare Data\n",
        "We will load the dataset and clean it. For demonstration purposes."
      ],
      "metadata": {
        "id": "DLAAjFXM9ad7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1GSRJgJaRzb1OauKtHdwNoF4UnkKAt9cK'\n",
        "output = 'Multilingual-Dataset.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "6hiCPo0m1rJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "if zipfile.is_zipfile(output):\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir('/content/'))\n",
        "else:\n",
        "    print(\"Downloaded file is not a zip file.\")"
      ],
      "metadata": {
        "id": "dKtQxZMr1yCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Data Prepration\n",
        "we need to organize the text files in the `SourceTexts` directory into subdirectories based on language identifiers extracted from the filenames. It dynamically identifies unique language codes, creates corresponding language-specific folders, and moves each file to its appropriate folder, thereby streamlining file management and access based on language.\n",
        "\n",
        "- Arabic\n",
        "- Czech\n",
        "- English\n",
        "- French\n",
        "- Greek\n",
        "- Hebrew\n",
        "- Hindi"
      ],
      "metadata": {
        "id": "6ery_95JqxUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source directory\n",
        "source_dir = 'SourceTexts'\n",
        "\n",
        "# List all files in the source directory\n",
        "files = os.listdir(source_dir)\n",
        "\n",
        "# Create a set to store unique language identifiers\n",
        "languages = set()\n",
        "\n",
        "# Identify unique languages from filenames\n",
        "for file in files:\n",
        "    language = file.split('.')[1].lower()  # Normalize to lowercase\n",
        "    languages.add(language)\n",
        "\n",
        "# Create folders for each language and move corresponding files\n",
        "for language in languages:\n",
        "    # Create the language folder if it doesn't exist\n",
        "    language_dir = os.path.join(source_dir, language)\n",
        "    os.makedirs(language_dir, exist_ok=True)\n",
        "\n",
        "    # Move files into the language folder\n",
        "    for file in files:\n",
        "        if file.split('.')[1].lower() == language:  # Normalize to lowercase\n",
        "            shutil.move(os.path.join(source_dir, file), os.path.join(language_dir, file))\n",
        "\n",
        "print(\"Files have been organized into folders.\")\n"
      ],
      "metadata": {
        "id": "LHwG8HdB3YaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we combine the content of text files from the `SourceTexts` directory into a single Pandas DataFrame, with each file's content stored as a row in the DataFrame. It then saves the DataFrame to a CSV file.\n",
        "\n",
        "1. **Import Required Modules**:\n",
        "   - The script imports the `os` module for directory and file operations and the `pandas` module for creating and manipulating the DataFrame."
      ],
      "metadata": {
        "id": "y1kdy1KXtfHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NepzRZDjtgpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Define the Function to Combine Files**:\n",
        "   - A function named `combine_files_to_dataframe` is defined. This function takes a directory path as input and returns a DataFrame containing the file names and their contents.\n",
        "- **Initialize an Empty List**: An empty list named `data` is created to store the file names and their contents.\n",
        "- **Loop Through Files**: The function loops through all files in the specified directory using `os.listdir()`.\n",
        "- **Construct Full File Path**: For each file, the full path is constructed using `os.path.join()`.\n",
        "- **Check if it’s a File**: The script ensures the path is a file using `os.path.isfile()`.\n",
        "- **Read File Content**: If it’s a file, the script reads its content and appends a dictionary containing the file name (`Title`) and its content (`Content`) to the `data` list.\n",
        "- **Create a DataFrame**: Finally, a Pandas DataFrame is created from the `data` list and returned."
      ],
      "metadata": {
        "id": "He6MLZTWtlUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_files_to_dataframe(directory_path):\n",
        "       # Initialize an empty list to store the data\n",
        "       data = []\n",
        "\n",
        "       # Loop through all files in the directory\n",
        "       for file_name in os.listdir(directory_path):\n",
        "           # Construct full file path\n",
        "           file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "           # Ensure it's a file\n",
        "           if os.path.isfile(file_path):\n",
        "               # Read the content of each file\n",
        "               with open(file_path, 'r') as file:\n",
        "                   content = file.read()\n",
        "                   # Append the file name and content to the data list\n",
        "                   data.append({'Title': file_name, 'Content': content.strip()})\n",
        "\n",
        "       # Create a DataFrame from the data list\n",
        "       df = pd.DataFrame(data)\n",
        "       return df"
      ],
      "metadata": {
        "id": "nLuQuiQltps_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. **Specify the Directory Path**:\n",
        "   - The directory containing the files is specified. In this example, the directory path is `/content/SourceTexts/english`."
      ],
      "metadata": {
        "id": "5QT8Bln6t-I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/SourceTexts/english' ## here you can change the language"
      ],
      "metadata": {
        "id": "sSNFdIR1t_pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Combine Files into a DataFrame**:\n",
        "   - The `combine_files_to_dataframe` function is called with the specified directory path, and the resulting DataFrame is stored in the variable `df`."
      ],
      "metadata": {
        "id": "P_IU-C7auL5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = combine_files_to_dataframe(directory_path)"
      ],
      "metadata": {
        "id": "4V6C6E63uOug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Display the DataFrame**:\n",
        "   - The DataFrame is printed to the console to display the combined content of the files."
      ],
      "metadata": {
        "id": "qQYOjKmtuRVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "uwnQd0yPuQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "6. **Save the DataFrame to a CSV File**:\n",
        "   - The DataFrame is saved to a CSV file named `combined_files.csv` without the index using the `to_csv` method."
      ],
      "metadata": {
        "id": "8lhJccJ0uXWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('combined_files.csv', index=False)"
      ],
      "metadata": {
        "id": "hXhQ30hOueJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "> The following code cell is designed to perform the combined operations described above. Let's execute it to see the results.\n",
        "---"
      ],
      "metadata": {
        "id": "WN4pRZj7u0pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def combine_files_to_dataframe(directory_path):\n",
        "    # Initialize an empty list to store the data\n",
        "    data = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        # Construct full file path\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "        # Ensure it's a file\n",
        "        if os.path.isfile(file_path):\n",
        "            # Read the content of each file\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                # Append the file name and content to the data list\n",
        "                data.append({'Title': file_name, 'Content': content.strip()})\n",
        "\n",
        "    # Create a DataFrame from the data list\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Specify the directory containing the files\n",
        "directory_path = '/content/SourceTexts/english'\n",
        "\n",
        "# Combine the files into a DataFrame\n",
        "df = combine_files_to_dataframe(directory_path)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# save the DataFrame to a CSV file\n",
        "df.to_csv('combined_files.csv', index=False)"
      ],
      "metadata": {
        "id": "ufC-8Qqu18K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Data Cleaning\n",
        "\n",
        "Enhances the readability and usability of text data by:\n",
        "1. Removing HTML tags using the `BeautifulSoup` library.\n",
        "2. Cleaning the text with regular expressions to replace multiple spaces, remove standalone numbers, and trim whitespace.\n",
        "3. Loading the original text data from a CSV file into a DataFrame.\n",
        "4. Applying the text cleaning process to the 'Content' column and storing the cleaned text in a new column named 'clean_content'."
      ],
      "metadata": {
        "id": "D0jQvrfBvd_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "L6oCkvDXwNQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Define a Function to Clean HTML**:\n",
        "   - A function named `clean_html` is defined to remove HTML tags from a string. It uses `BeautifulSoup` to parse the HTML and extract the text content.\n",
        "- **Parameter**: `raw_html` - The raw HTML string to be cleaned.\n",
        "- **Processing**: `BeautifulSoup` parses the HTML and the `get_text()` method extracts the text content.\n",
        "- **Return**: The cleaned text without HTML tags."
      ],
      "metadata": {
        "id": "p5PPKuh_wjCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_html(raw_html):\n",
        "       \"\"\"Remove HTML tags from a string\"\"\"\n",
        "       soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "       return soup.get_text()"
      ],
      "metadata": {
        "id": "bLpxCazWwlVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Define a Function to Clean Text**:\n",
        "  - A function named `clean_text` is defined to clean the text by removing unwanted patterns and extra spaces. It first removes HTML tags using the `clean_html` function and then applies various regular expressions to clean the text further.\n",
        "- **Parameter**: `text` - The text string to be cleaned.\n",
        "- **Processing**:\n",
        "     - `clean_html(text)`: Removes HTML tags.\n",
        "     - `re.sub(r'\\s+', ' ', text)`: Replaces multiple spaces and newlines with a single space.\n",
        "     - `re.sub(r'\\b\\d+\\b', '', text)`: Removes standalone numbers (optional, based on specific needs).\n",
        "     - `text.strip()`: Removes leading and trailing spaces.\n",
        "- **Return**: The cleaned text."
      ],
      "metadata": {
        "id": "trJlxZ0aw2Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "       \"\"\"Remove unwanted text patterns and clean the text\"\"\"\n",
        "       text = clean_html(text)\n",
        "       text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
        "       text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers if needed\n",
        "       text = text.strip()  # Remove leading/trailing spaces\n",
        "       return text"
      ],
      "metadata": {
        "id": "UzR-XxpOwXw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Load the DataFrame**:\n",
        "   - The script loads a DataFrame from a CSV file named `combined_files.csv` using the `pandas` library.\n"
      ],
      "metadata": {
        "id": "kHu3xlhkxROP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('combined_files.csv')"
      ],
      "metadata": {
        "id": "8d8j1Ji6wess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Apply Preprocessing to the 'Content' Column**:\n",
        "   - The script applies the `clean_text` function to each entry in the 'Content' column of the DataFrame, creating a new column 'clean_content' with the cleaned text.\n",
        "- **Processing**: The `apply` method is used to apply the `clean_text` function to each row in the 'Content' column.\n",
        "- **Result**: A new column 'clean_content' is added to the DataFrame containing the cleaned text."
      ],
      "metadata": {
        "id": "W_pmFCQxxWeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['Content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "N-l4X1mcxapj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "> The following code cell is designed to perform the combined operations described above. Let's execute it to see the results.\n",
        "---"
      ],
      "metadata": {
        "id": "j-kfX5fpxp4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_html(raw_html):\n",
        "    \"\"\"Remove HTML tags from a string\"\"\"\n",
        "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted text patterns and clean the text\"\"\"\n",
        "    text = clean_html(text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
        "    text = re.sub(r'\\b\\d+\\b', '', text)  # Remove standalone numbers if needed\n",
        "    text = text.strip()  # Remove leading/trailing spaces\n",
        "    return text\n",
        "\n",
        "\n",
        "# Load your DataFrame\n",
        "df = pd.read_csv('combined_files.csv')\n",
        "\n",
        "# Apply preprocessing to the 'Content' column\n",
        "df['clean_content'] = df['Content'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "xNJk4rDivfAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load the SpaCy Model and Tokenizer\n",
        "Load the SpaCy English model, which will be used for tokenization, parsing, and named entity recognition. Recreate the tokenizer with the updated stopwords list.\n",
        "    \n"
      ],
      "metadata": {
        "id": "__69eet8-hs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "import pandas as pd\n",
        "from tabulate import tabulate"
      ],
      "metadata": {
        "id": "808lQRfC-z6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = Tokenizer(nlp.vocab)"
      ],
      "metadata": {
        "id": "G-70wk1p-llV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 5: Tokenize and Filter Text\n",
        "- The `tokenize_and_filter` function processes text to remove stop words, punctuation, and empty tokens.\n",
        "- This function is applied to a DataFrame column `'clean_content'`, resulting in a new column of filtered tokens `'tokens'`.\n"
      ],
      "metadata": {
        "id": "AjYg_Pwc_FZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_filter(text):\n",
        "    \"\"\"Tokenize the text and filter out stop words, punctuation, and empty tokens\n",
        "      - **Input:** A string of text.\n",
        "      - **Output:** A list of filtered tokens (words)\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct and token.text.strip()]\n",
        "    return tokens\n",
        "\n",
        "# Apply tokenization to the 'clean_content' column\n",
        "df['tokens'] = df['clean_content'].apply(tokenize_and_filter)\n",
        "\n",
        "# Display the DataFrame with tokens\n",
        "print(df[['Title', 'tokens']])\n"
      ],
      "metadata": {
        "id": "7pZqTQfq5NLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 5: Create a Directory for Plots\n",
        "Create a new directory named 'plots' to store generated plots. This helps keep the workspace organized.\n",
        "    \n"
      ],
      "metadata": {
        "id": "hD1dpX7L_IqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir plots"
      ],
      "metadata": {
        "id": "g-yBLuMH_Lbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get casting Image"
      ],
      "metadata": {
        "id": "cwXmrfHdzAaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1WoIBqqLufm9Ou3toI5uKVIcRuC-_Qswh'\n",
        "output = 'Mic.png'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "rgD6YhaCygje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Get Font:"
      ],
      "metadata": {
        "id": "xqHxCtHf0eMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1HwUK9hixOUcF_8mrpP2nD4zbyMA0utz2'\n",
        "output = 'font.ttf'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "G4dAZxyd0hdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def visualize_word_cloud(texts, mask_path, font_path):\n",
        "    # Combine all texts into one string\n",
        "    all_words = ' '.join(texts)\n",
        "\n",
        "    # Load a mask image for custom shape\n",
        "    mask = np.array(Image.open(mask_path))  # mask image path\n",
        "\n",
        "    # Define stop words\n",
        "    stopwords = set(STOPWORDS)\n",
        "\n",
        "    # Create a WordCloud object with additional customization\n",
        "    wordcloud = WordCloud(\n",
        "        font_path=font_path,\n",
        "        background_color='white',\n",
        "        max_words=200,\n",
        "        stopwords=stopwords,\n",
        "        mask=mask,  # Use mask for custom shape\n",
        "        contour_width=3,\n",
        "        contour_color='firebrick',\n",
        "        colormap='viridis',  # Color theme\n",
        "        width=800,\n",
        "        height=400,\n",
        "        random_state=42\n",
        "    ).generate(all_words)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('plots/Word_Cloud _tokens.png')\n",
        "    plt.show()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_word_cloud(df['clean_content'], 'Mic.png', 'font.ttf')\n"
      ],
      "metadata": {
        "id": "ger7-rWV_VaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 6: Extract Linguistic Annotations\n",
        "Extract part of speech tags and dependencies for each token and display them in a DataFrame.\n",
        "    \n"
      ],
      "metadata": {
        "id": "gSGWmama_M5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine all tokens in the corpus\n",
        "all_tokens = ' '.join(df['clean_content'].apply(remove_stop_words)).split()\n",
        "\n",
        "# Get the top N most frequent words\n",
        "N = 20\n",
        "most_common_tokens = Counter(all_tokens).most_common(N)\n",
        "\n",
        "# Plot the top N most frequent words\n",
        "tokens, counts = zip(*most_common_tokens)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(tokens, counts)\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top N Most Frequent Words in the Corpus')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig('plots/top_20_tokens.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qNgyc0am_fuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity Recognition Analysis"
      ],
      "metadata": {
        "id": "pvLXeeVMw52y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic Dependency Analysis"
      ],
      "metadata": {
        "id": "iwGKi5_9xJhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 7: Visualize Syntactic Dependencies\n",
        "Use SpaCy's displaCy visualizer to create an interactive diagram of the sentence's syntactic dependencies.\n",
        "    "
      ],
      "metadata": {
        "id": "_xuN9qCX_X6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize syntactic dependencies of the first sentence of the first document\n",
        "from spacy import displacy\n",
        "def visualize_syntactic_dependencies(text):\n",
        "    doc = nlp(text)\n",
        "    first_sentence = list(doc.sents)[0]\n",
        "    displacy.render(first_sentence, style='dep', jupyter=True)\n",
        "\n",
        "# Visualize for the first document as an example\n",
        "visualize_syntactic_dependencies(df['clean_content'].iloc[0])\n"
      ],
      "metadata": {
        "id": "TE3M1EZHxKSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Named Entity Recognition (NER)\n",
        "Extract and display named entities from the text. Visualize entities using SpaCy's displaCy visualizer.\n",
        "    "
      ],
      "metadata": {
        "id": "B1irGwzw_aE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities from the corpus\n",
        "entity_labels = ['PERSON', 'ORG', 'GPE', 'LOC']\n",
        "entity_counts = Counter()\n",
        "\n",
        "for text in df['clean_content']:\n",
        "    doc = nlp(text)\n",
        "    entities = [ent.label_ for ent in doc.ents if ent.label_ in entity_labels]\n",
        "    entity_counts.update(entities)\n",
        "\n",
        "# Plot the frequency of different named entities\n",
        "labels, counts = zip(*entity_counts.items())\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, counts)\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Different Named Entities in the Corpus')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C7R8Kkl45ZRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Visualize Syntactic Dependencies and Extract Entity Details\n",
        "def visualize_and_extract_entities(text):\n",
        "    \"\"\"Visualize syntactic dependencies and extract entity details for the first sentence\"\"\"\n",
        "    doc = nlp(text)\n",
        "    first_sentence = list(doc.sents)[0]  # Get the first sentence\n",
        "    #displacy.render(first_sentence, style='dep', jupyter=True)\n",
        "    # Visualize entities\n",
        "    displacy.render(doc, style=\"ent\")\n",
        "\n",
        "    # Extract entity details\n",
        "    data = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
        "    headers = ['Entity', 'Start Char', 'End Char', 'Label']\n",
        "    print(tabulate(data, headers=headers, tablefmt=\"pretty\"))\n",
        "\n",
        "# Visualize and extract entities for the first sentence of the first document\n",
        "first_text = df['clean_content'].iloc[0]\n",
        "visualize_and_extract_entities(first_text)"
      ],
      "metadata": {
        "id": "ffJY20Gp_k3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **word tree** [(google charts) ](https://developers.google.com/chart/interactive/docs/gallery/wordtree)"
      ],
      "metadata": {
        "id": "WDgdJihytfx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Combine Titles and Content:**\n",
        "   The combined text is created by joining the `Title` and `clean_content` columns of your DataFrame. Ensure you handle any special characters that might interfere with JavaScript strings.\n",
        "\n",
        "2. **Generate Word Tree HTML:**\n",
        "   The `generate_word_tree_html` function creates the HTML and JavaScript necessary to render the Word Tree. It uses Google Charts to create the visualization. Note the handling of special characters within the `combined_text` to avoid breaking the JavaScript code.\n",
        "\n",
        "3. **Interactive Widget:**\n",
        "   An `ipywidgets.Text` widget is created for the user to input the search word. The `display_word_tree` function generates and displays the Word Tree whenever the input value changes.\n",
        "\n",
        "4. **Display the Initial Word Tree:**\n",
        "   The initial call to `display_word_tree(None)` ensures the Word Tree is displayed as soon as the notebook runs, with the default search word.\n"
      ],
      "metadata": {
        "id": "7BmoMtx0COck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.core.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "\n",
        "# Combine titles and summaries into a single text block for the Word Tree\n",
        "combined_text = '. '.join(df['Title'] + ': ' + df['clean_content'])\n",
        "\n",
        "# Additional sanitization function to handle special characters\n",
        "def sanitize_text(text):\n",
        "    text = text.replace('\"', '\\\\\"')\n",
        "    text = text.replace(\"'\", \"\\\\'\")\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = text.replace('\\\\', '\\\\\\\\')\n",
        "    return text\n",
        "\n",
        "# Sanitize combined text\n",
        "safe_text = sanitize_text(combined_text)\n",
        "\n",
        "\n",
        "# Function to generate the HTML and JavaScript for the Word Tree\n",
        "def generate_word_tree_html(search_word):\n",
        "    # Limit text length to avoid exceeding browser's JavaScript limits\n",
        "    safe_text_chunk = safe_text[:1000]\n",
        "    print(\"Using Text Chunk (first 1000 chars):\", safe_text_chunk[:1000])  # Debug: Print the first 1000 characters of the chunk\n",
        "\n",
        "    return f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "      <head>\n",
        "        <script type=\"text/javascript\" src=\"https://www.gstatic.com/charts/loader.js\"></script>\n",
        "        <script type=\"text/javascript\">\n",
        "          google.charts.load('current', {{packages:['wordtree']}});\n",
        "          google.charts.setOnLoadCallback(drawChart);\n",
        "          function drawChart() {{\n",
        "            var data = google.visualization.arrayToDataTable([\n",
        "              ['Phrases'],\n",
        "              [\"{safe_text_chunk}\"]\n",
        "            ]);\n",
        "\n",
        "            var options = {{\n",
        "              wordtree: {{\n",
        "                format: 'implicit',\n",
        "                type: 'double',\n",
        "                word: '{search_word}'\n",
        "              }}\n",
        "            }};\n",
        "\n",
        "            var chart = new google.visualization.WordTree(document.getElementById('wordtree_basic'));\n",
        "            chart.draw(data, options);\n",
        "          }}\n",
        "        </script>\n",
        "      </head>\n",
        "      <body>\n",
        "        <div id=\"wordtree_basic\" style=\"width: 100%; height: 600px;\"></div>\n",
        "      </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "# Create an input widget\n",
        "search_word_input = widgets.Text(\n",
        "    value='the',\n",
        "    placeholder='Enter a search word',\n",
        "    description='Search word:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Function to display the Word Tree\n",
        "def display_word_tree(change):\n",
        "    search_word = search_word_input.value\n",
        "    word_tree_html = generate_word_tree_html(search_word)\n",
        "    display(HTML(word_tree_html))\n",
        "\n",
        "# Attach the function to the input widget\n",
        "search_word_input.observe(display_word_tree, names='value')\n",
        "\n",
        "# Display the input widget\n",
        "display(search_word_input)\n",
        "\n",
        "# Initial display of the Word Tree\n",
        "display_word_tree(None)\n"
      ],
      "metadata": {
        "id": "O0nRi4BaAoh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx matplotlib spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "uJFBrsnKA1rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> **For demonstration purposes, we will use the summarised data here.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "aUR07nYMtEvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1OmIg0exqU7kirt6SDeRKFkLTFPVu41ws'\n",
        "output = 'summarized_data.csv'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "ZMhy8j9esdU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "df = pd.read_csv('/content/summarized_data.csv')\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Extract entities\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Create a DataFrame for entities\n",
        "entities_df = pd.DataFrame(entities, columns=['Entity', 'Label'])\n",
        "\n",
        "# Show the first few rows of entities_df\n",
        "print(entities_df.head())\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for entity in doc.ents:\n",
        "    G.add_node(entity.text, label=entity.label_)\n",
        "    for token in entity.root.head.subtree:\n",
        "        if token != entity.root and token.ent_type_:\n",
        "            G.add_edge(entity.text, token.text)\n",
        "\n",
        "# Draw the network graph\n",
        "plt.figure(figsize=(15, 10))\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", edge_color=\"#BBBBBB\")\n",
        "plt.title('Network Graph of Entities')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AQ2gcKYdHO1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on entities and POS tags\n",
        "for token in doc:\n",
        "    if token.ent_type_:\n",
        "        # Add POS tag as a node attribute\n",
        "        G.add_node(token.text, label=token.ent_type_, POS=token.pos_)\n",
        "        for child in token.children:\n",
        "            if child.ent_type_:\n",
        "                G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='YlGnBu',\n",
        "        size=10,\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Connections',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line_width=2))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Interactive Network Graph of Entities and POS',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[ dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TWXEryRjHReX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Define specific entities to highlight\n",
        "highlight_entities = [\"Haiti\", \"earthquake\", \"communication\"]\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on entities and POS tags\n",
        "for token in doc:\n",
        "    if token.ent_type_ or token.text in highlight_entities:\n",
        "        # Add POS tag as a node attribute\n",
        "        G.add_node(token.text, label=token.ent_type_, POS=token.pos_, color='red' if token.text in highlight_entities else 'skyblue')\n",
        "        for child in token.children:\n",
        "            if child.ent_type_ or child.text in highlight_entities:\n",
        "                G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "node_color = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "    node_color.append(G.nodes[node]['color'])\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='YlGnBu',\n",
        "        size=10,\n",
        "        color=node_color,\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Connections',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line_width=2))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Interactive Network Graph of Entities and POS with Highlights',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[ dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "J5MQXwdxHfIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Show Network Graph for Specific Words"
      ],
      "metadata": {
        "id": "DBHgy9FsHmoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Data Preparation**:\n",
        "   - The `clean_html` function removes HTML tags.\n",
        "   - The `clean_text` function removes extra spaces, standalone numbers, and trims leading/trailing spaces.\n",
        "   - The `summa_summarize` function summarizes the cleaned text.\n",
        "\n",
        "2. **Entity Extraction**:\n",
        "   - Use Spacy to process the combined text and extract entities.\n",
        "\n",
        "3. **Build Graph**:\n",
        "   - Create a network graph using `networkx`.\n",
        "   - Add nodes for each entity and edges to show relationships.\n",
        "\n",
        "4. **Visualize**:\n",
        "   - Use `matplotlib` to visualize the network graph."
      ],
      "metadata": {
        "id": "ats23sb_H50H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on specific words and their connections\n",
        "specific_words = ['government', 'people','Qaeda']\n",
        "\n",
        "for token in doc:\n",
        "    if token.text in specific_words:\n",
        "        G.add_node(token.text, label=token.pos_)\n",
        "        for child in token.children:\n",
        "            G.add_node(child.text, label=child.pos_)\n",
        "            G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        showscale=True,\n",
        "        colorscale='YlGnBu',\n",
        "        size=10,\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Connections',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line_width=2))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Interactive Network Graph for Specific Words',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[ dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "EBy_peOoHsYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on entities and POS tags\n",
        "highlight_entity = 'PERSON'  # Change to the entity type you want to highlight\n",
        "\n",
        "for token in doc:\n",
        "    if token.ent_type_:\n",
        "        # Add POS tag as a node attribute\n",
        "        G.add_node(token.text, label=token.ent_type_, POS=token.pos_)\n",
        "        for child in token.children:\n",
        "            if child.ent_type_:\n",
        "                G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=1.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "node_color = []\n",
        "node_size = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "    if G.nodes[node]['label'] == highlight_entity:\n",
        "        node_color.append('red')\n",
        "        node_size.append(20)\n",
        "    else:\n",
        "        node_color.append('skyblue')\n",
        "        node_size.append(10)\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        color=node_color,\n",
        "        size=node_size,\n",
        "        line=dict(width=2),\n",
        "        showscale=True,\n",
        "        colorscale='YlGnBu',\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Connections',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        )))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Interactive Network Graph of Entities with Highlighted Entities',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N53ecUtpIBKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on entities and POS tags\n",
        "highlight_entity = 'PERSON'  # Change to the entity type you want to highlight\n",
        "\n",
        "for token in doc:\n",
        "    if token.ent_type_:\n",
        "        # Add POS tag as a node attribute\n",
        "        G.add_node(token.text, label=token.ent_type_, POS=token.pos_)\n",
        "        for child in token.children:\n",
        "            if child.ent_type_:\n",
        "                G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.3)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=1.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "node_color = []\n",
        "node_size = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "    degree = G.degree[node]\n",
        "    if G.nodes[node]['label'] == highlight_entity:\n",
        "        node_color.append('red')\n",
        "        node_size.append(15 + degree * 2)\n",
        "    else:\n",
        "        node_color.append(degree)\n",
        "        node_size.append(10 + degree * 2)\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        color=node_color,\n",
        "        size=node_size,\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Degree',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line=dict(width=2)),\n",
        "    textfont=dict(\n",
        "        size=12,\n",
        "        color='black'\n",
        "    ))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Enhanced Interactive Network Graph of Entities with Highlighted Entities',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Word Search Widget\n",
        "def word_search_callback(b):\n",
        "    search_word = word_search_input.value.lower()\n",
        "    if not search_word:\n",
        "        return\n",
        "\n",
        "    # Find contexts for the search word\n",
        "    word_contexts = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() == search_word:\n",
        "            for child in token.children:\n",
        "                word_contexts.append(child.text.lower())\n",
        "            for ancestor in token.ancestors:\n",
        "                word_contexts.append(ancestor.text.lower())\n",
        "\n",
        "    # Count word frequencies before and after the search word\n",
        "    word_counts = Counter(word_contexts)\n",
        "    most_common_words = word_counts.most_common(10)\n",
        "\n",
        "    if most_common_words:\n",
        "        words, counts = zip(*most_common_words)\n",
        "    else:\n",
        "        words, counts = [], []\n",
        "\n",
        "    # Create bar chart for most common words around the search word\n",
        "    bar_chart = go.Figure([go.Bar(x=words, y=counts)])\n",
        "    bar_chart.update_layout(title=f\"Most Frequent Words Around '{search_word}'\",\n",
        "                            xaxis_title=\"Words\",\n",
        "                            yaxis_title=\"Frequency\")\n",
        "    bar_chart.show()\n",
        "\n",
        "# Create text input and button widgets\n",
        "word_search_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter a search word',\n",
        "    description='Search word:',\n",
        "    disabled=False\n",
        ")\n",
        "word_search_button = widgets.Button(\n",
        "    description='Search',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to search for the word'\n",
        ")\n",
        "\n",
        "# Set up the button click event\n",
        "word_search_button.on_click(word_search_callback)\n",
        "\n",
        "# Display the widgets\n",
        "display(word_search_input)\n",
        "display(word_search_button)\n"
      ],
      "metadata": {
        "id": "8VqrvbthID99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Create a network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes and edges based on entities and POS tags\n",
        "highlight_entity = 'PERSON'  # Change to the entity type you want to highlight\n",
        "\n",
        "for token in doc:\n",
        "    if token.ent_type_:\n",
        "        # Add POS tag as a node attribute\n",
        "        G.add_node(token.text, label=token.ent_type_, POS=token.pos_)\n",
        "        for child in token.children:\n",
        "            if child.ent_type_:\n",
        "                G.add_edge(token.text, child.text)\n",
        "\n",
        "# Generate positions for the nodes\n",
        "pos = nx.spring_layout(G, k=0.3)\n",
        "\n",
        "# Assign positions to the nodes in the graph\n",
        "for node in G.nodes:\n",
        "    G.nodes[node]['pos'] = pos[node]\n",
        "\n",
        "# Extract edge traces for visualization\n",
        "edge_x = []\n",
        "edge_y = []\n",
        "\n",
        "for edge in G.edges():\n",
        "    x0, y0 = G.nodes[edge[0]]['pos']\n",
        "    x1, y1 = G.nodes[edge[1]]['pos']\n",
        "    edge_x.append(x0)\n",
        "    edge_x.append(x1)\n",
        "    edge_x.append(None)\n",
        "    edge_y.append(y0)\n",
        "    edge_y.append(y1)\n",
        "    edge_y.append(None)\n",
        "\n",
        "edge_trace = go.Scatter(\n",
        "    x=edge_x, y=edge_y,\n",
        "    line=dict(width=1.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines')\n",
        "\n",
        "# Extract node traces for visualization\n",
        "node_x = []\n",
        "node_y = []\n",
        "node_text = []\n",
        "node_color = []\n",
        "node_size = []\n",
        "\n",
        "for node in G.nodes:\n",
        "    x, y = G.nodes[node]['pos']\n",
        "    node_x.append(x)\n",
        "    node_y.append(y)\n",
        "    node_text.append(f\"{node} ({G.nodes[node]['label']})\")\n",
        "    degree = G.degree[node]\n",
        "    if G.nodes[node]['label'] == highlight_entity:\n",
        "        node_color.append('red')\n",
        "        node_size.append(15 + degree * 2)\n",
        "    else:\n",
        "        node_color.append(degree)\n",
        "        node_size.append(10 + degree * 2)\n",
        "\n",
        "node_trace = go.Scatter(\n",
        "    x=node_x, y=node_y,\n",
        "    mode='markers+text',\n",
        "    text=node_text,\n",
        "    textposition='top center',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        color=node_color,\n",
        "        size=node_size,\n",
        "        colorscale='Viridis',\n",
        "        colorbar=dict(\n",
        "            thickness=15,\n",
        "            title='Node Degree',\n",
        "            xanchor='left',\n",
        "            titleside='right'\n",
        "        ),\n",
        "        line=dict(width=2)),\n",
        "    textfont=dict(\n",
        "        size=12,\n",
        "        color='black'\n",
        "    ))\n",
        "\n",
        "# Create plotly figure\n",
        "fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                layout=go.Layout(\n",
        "                    title='Enhanced Interactive Network Graph of Entities with Highlighted Entities',\n",
        "                    titlefont_size=16,\n",
        "                    showlegend=False,\n",
        "                    hovermode='closest',\n",
        "                    margin=dict(b=20,l=5,r=5,t=40),\n",
        "                    annotations=[dict(\n",
        "                        text=\"\",\n",
        "                        showarrow=False,\n",
        "                        xref=\"paper\", yref=\"paper\")],\n",
        "                    xaxis=dict(showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Word Search Widget\n",
        "def word_search_callback(b):\n",
        "    search_word = word_search_input.value.lower()\n",
        "    if not search_word:\n",
        "        return\n",
        "\n",
        "    # Find contexts for the search word\n",
        "    word_contexts = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() == search_word:\n",
        "            for child in token.children:\n",
        "                word_contexts.append(child.text.lower())\n",
        "            for ancestor in token.ancestors:\n",
        "                word_contexts.append(ancestor.text.lower())\n",
        "\n",
        "    # Count word frequencies before and after the search word\n",
        "    word_counts = Counter(word_contexts)\n",
        "    most_common_words = word_counts.most_common(10)\n",
        "\n",
        "    if most_common_words:\n",
        "        words, counts = zip(*most_common_words)\n",
        "    else:\n",
        "        words, counts = [], []\n",
        "\n",
        "    # Create a network graph for the search word\n",
        "    search_graph = nx.Graph()\n",
        "    search_graph.add_node(search_word, size=40)\n",
        "\n",
        "    for word, count in most_common_words:\n",
        "        search_graph.add_node(word, size=20 + count*2)\n",
        "        search_graph.add_edge(search_word, word, weight=count)\n",
        "\n",
        "    pos = nx.spring_layout(search_graph)\n",
        "\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "\n",
        "    for edge in search_graph.edges():\n",
        "        x0, y0 = pos[edge[0]]\n",
        "        x1, y1 = pos[edge[1]]\n",
        "        edge_x.append(x0)\n",
        "        edge_x.append(x1)\n",
        "        edge_x.append(None)\n",
        "        edge_y.append(y0)\n",
        "        edge_y.append(y1)\n",
        "        edge_y.append(None)\n",
        "\n",
        "    edge_trace = go.Scatter(\n",
        "        x=edge_x, y=edge_y,\n",
        "        line=dict(width=1.5, color='#888'),\n",
        "        hoverinfo='none',\n",
        "        mode='lines')\n",
        "\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_text = []\n",
        "    node_size = []\n",
        "\n",
        "    for node in search_graph.nodes():\n",
        "        x, y = pos[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        node_text.append(node)\n",
        "        node_size.append(search_graph.nodes[node]['size'])\n",
        "\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y,\n",
        "        mode='markers+text',\n",
        "        text=node_text,\n",
        "        textposition='top center',\n",
        "        hoverinfo='text',\n",
        "        marker=dict(\n",
        "            size=node_size,\n",
        "            colorscale='YlGnBu',\n",
        "            colorbar=dict(\n",
        "                thickness=15,\n",
        "                title='Node Connections',\n",
        "                xanchor='left',\n",
        "                titleside='right'\n",
        "            ),\n",
        "            line=dict(width=2)),\n",
        "        textfont=dict(\n",
        "            size=12,\n",
        "            color='black'\n",
        "        ))\n",
        "\n",
        "    search_fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=f'Network Graph for \"{search_word}\"',\n",
        "                        titlefont_size=16,\n",
        "                        showlegend=False,\n",
        "                        hovermode='closest',\n",
        "                        margin=dict(b=20,l=5,r=5,t=40),\n",
        "                        annotations=[dict(\n",
        "                            text=\"\",\n",
        "                            showarrow=False,\n",
        "                            xref=\"paper\", yref=\"paper\")],\n",
        "                        xaxis=dict(showgrid=False, zeroline=False),\n",
        "                        yaxis=dict(showgrid=False, zeroline=False)))\n",
        "\n",
        "    search_fig.show()\n",
        "\n",
        "# Create text input and button widgets\n",
        "word_search_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter a search word',\n",
        "    description='Search word:',\n",
        "    disabled=False\n",
        ")\n",
        "word_search_button = widgets.Button(\n",
        "    description='Search',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Click to search for the word'\n",
        ")\n",
        "\n",
        "# Set up the button click event\n",
        "word_search_button.on_click(word_search_callback)\n",
        "\n",
        "# Display the widgets\n",
        "display(word_search_input)\n",
        "display(word_search_button)\n"
      ],
      "metadata": {
        "id": "m536HfOuIHUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "SpEJmQM4IOl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, IFrame\n",
        "from pyvis.network import Network\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine all summaries into a single text block for analysis\n",
        "combined_text = ' '.join(df['summary_transformer'])\n",
        "\n",
        "# Process text with Spacy\n",
        "doc = nlp(combined_text)\n",
        "\n",
        "# Word Search Widget\n",
        "def word_search_callback(b):\n",
        "    search_word = word_search_input.value.lower()\n",
        "    if not search_word:\n",
        "        return\n",
        "\n",
        "    # Find contexts for the search word\n",
        "    word_contexts = []\n",
        "    for token in doc:\n",
        "        if token.text.lower() == search_word:\n",
        "            for child in token.children:\n",
        "                word_contexts.append(child.text.lower())\n",
        "            for ancestor in token.ancestors:\n",
        "                word_contexts.append(ancestor.text.lower())\n",
        "\n",
        "    # Count word frequencies before and after the search word\n",
        "    word_counts = Counter(word_contexts)\n",
        "    most_common_words = word_counts.most_common(10)\n",
        "\n",
        "    if most_common_words:\n",
        "        words, counts = zip(*most_common_words)\n",
        "    else:\n",
        "        words, counts = [], []\n",
        "\n",
        "    # Debug: Print the most common words and counts\n",
        "    print(\"Most Common Words and Counts:\", list(zip(words, counts)))\n",
        "\n",
        "    # Create a network graph for the search word\n",
        "    search_graph = nx.Graph()\n",
        "    search_graph.add_node(search_word, size=60)\n",
        "\n",
        "    for word, count in most_common_words:\n",
        "        search_graph.add_node(word, size=20 + count*2)\n",
        "        search_graph.add_edge(search_word, word, weight=count)\n",
        "\n",
        "    # Debug: Print nodes and edges\n",
        "    print(\"Nodes:\", search_graph.nodes(data=True))\n",
        "    print(\"Edges:\", search_graph.edges(data=True))\n",
        "\n",
        "    # Create interactive network graph using pyvis\n",
        "    net = Network(notebook=True, height='750px', width='100%', cdn_resources='in_line')\n",
        "\n",
        "    # Add the main node\n",
        "    net.add_node(search_word, label=search_word, title=search_word, color='gray', size=60, font={'size': 32, 'face': 'Arial'})\n",
        "\n",
        "    # Add nodes and edges\n",
        "    for word, count in most_common_words:\n",
        "        node_color = 'blue'\n",
        "        node_size = 30 + count * 5\n",
        "        font_size = max(6, int(node_size / 2))\n",
        "        edge_length = 1 / count  # Inversely proportional to frequency\n",
        "        net.add_node(word, label=word, title=word, color=node_color, size=node_size, font={'size': font_size, 'face': 'Arial'})\n",
        "        net.add_edge(search_word, word, value=count, title=f'Weight: {count}', color='grey', length=edge_length)\n",
        "\n",
        "    net.show(\"network_graph.html\")\n",
        "\n",
        "    display(IFrame('network_graph.html', width=750, height=750))\n",
        "\n",
        "# Create text input and button widgets\n",
        "word_search_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter a search word',\n",
        "    description='Search word:',\n",
        "    disabled=False\n",
        ")\n",
        "word_search_button = widgets.Button(\n",
        "    description='Search',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Click to search for the word'\n",
        ")\n",
        "\n",
        "# Set up the button click event\n",
        "word_search_button.on_click(word_search_callback)\n",
        "\n",
        "# Display the widgets\n",
        "display(word_search_input)\n",
        "display(word_search_button)\n"
      ],
      "metadata": {
        "id": "2q0fCUo0IQIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "with open('network_graph.html', 'r') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "# Step 3: Display the HTML content in Colab\n",
        "display(HTML(html_content))\n"
      ],
      "metadata": {
        "id": "8Zk8xIDasJHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFKkyEWnsWn9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
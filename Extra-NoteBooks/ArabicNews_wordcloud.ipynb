{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaDq4QwfrqASkVowqAZRCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCREL/Session1_Visualisation_and_Summarisation/blob/main/Extra-NoteBooks/ArabicNews_wordcloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Arabic News\n",
        "\n",
        "Arabic news crawled from BBC, CNN, JSC, RT, and EuroNews\n",
        "https://www.kaggle.com/datasets/mksaad/arabic-news/data"
      ],
      "metadata": {
        "id": "Phw9WxbxMt7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mglearn\n",
        "!pip install gdown\n",
        "!pip install wordcloud\n",
        "!pip install plotly\n",
        "!pip install spacy\n",
        "!pip install spacy-langdetect\n",
        "!pip install langdetect\n"
      ],
      "metadata": {
        "id": "bfsqx-BqkIUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ6SnuIAMP6l"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1REOz6rfcjPuIhIimsrpBwhifvmI9gCTx'\n",
        "output = 'Arabic_news.zip'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "if zipfile.is_zipfile(output):\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(\"Files extracted:\")\n",
        "    print(os.listdir('/content/'))\n",
        "else:\n",
        "    print(\"Downloaded file is not a zip file.\")"
      ],
      "metadata": {
        "id": "rtp2J42KMTRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Arabic_news.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ANcB_wR5NwL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "_Ak7zN5RN-6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "XbEgcEqSO27T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum() #no missing values"
      ],
      "metadata": {
        "id": "WuIWc2K3PXbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df.loc[:,['description', 'source_domain','url']]"
      ],
      "metadata": {
        "id": "eyDaq4vuQlPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3"
      ],
      "metadata": {
        "id": "OVQmNIPfRA8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df3.rename(columns = {'description' : 'text'})"
      ],
      "metadata": {
        "id": "yfkzkxBBRD7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3['source_domain'].value_counts().nlargest(20).plot(kind = 'bar')"
      ],
      "metadata": {
        "id": "A3kXo2ZPRi4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df3.groupby('source_domain').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "metadata": {
        "id": "t-JMU5HMRo3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Use Plotly for visualisation\n",
        "fig = px.bar(df3.groupby('source_domain').size().sort_values(ascending=False).reset_index(),\n",
        "             x='source_domain', y=0,\n",
        "             labels={'0': 'Count'},\n",
        "             color='source_domain',  # Color bars based on source domain\n",
        "             color_discrete_sequence=px.colors.qualitative.Pastel)  # Use a color palette\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Distribution of News Sources',\n",
        "    xaxis_title='Source Domain',\n",
        "    yaxis_title='Count',\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "B-VC9tRpS5YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data prep"
      ],
      "metadata": {
        "id": "Oua2q-ikUfv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Preprocess the text data\n",
        "\n",
        "We will use spaCy to preprocess the text data, including tokenization and stop words removal."
      ],
      "metadata": {
        "id": "e89QRwyrUjua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the distribution of news sources\n",
        "source_counts = df['source_domain'].value_counts()\n",
        "print(source_counts)\n",
        "# Determine the minimum count of articles across all sources\n",
        "min_count = source_counts.min()\n",
        "print(f\"\\nMinimum count of articles across all sources: {min_count}\")\n",
        "\n",
        "# Create a balanced subset\n",
        "balanced_df = df.groupby('source_domain').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
        "\n",
        "# Display the distribution of news sources in the balanced subset\n",
        "balanced_source_counts = balanced_df['source_domain'].value_counts()\n",
        "print(\"\\nDistribution of news sources in the balanced subset:\")\n",
        "print(balanced_source_counts)\n",
        "\n",
        "# Save the balanced dataset to a new CSV file\n",
        "balanced_df.to_csv('BalancedArabicNewsDataset.csv', index=False)\n",
        "\n",
        "print(\"\\nBalanced dataset saved to 'BalancedArabicNewsDataset.csv'\")"
      ],
      "metadata": {
        "id": "wQQQc5Yzhuda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "id": "UvAiEa_UkmOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.ar import Arabic\n",
        "\n",
        "# Load spaCy Arabic model\n",
        "nlp = Arabic()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize and remove stop words\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "balanced_df['clean_text'] = balanced_df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\nFirst few rows of the cleaned dataset:\")\n",
        "balanced_df[['text', 'clean_text']].head()\n"
      ],
      "metadata": {
        "id": "RVUXkDE7UhDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Visualization\n",
        "\n",
        "### 2.1 Word Frequency Visualization\n",
        "\n",
        "https://www.wordclouds.co.uk/"
      ],
      "metadata": {
        "id": "vOArjDSIU4CC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Color Themes:**\n",
        "   - `colormap='viridis'` sets a color theme for the word cloud. You can choose from various colormaps provided by Matplotlib (e.g., 'viridis', 'plasma', 'inferno', 'magma').\n",
        "\n",
        "2. **Custom Shapes:**\n",
        "   - `mask=mask` uses a custom mask image to shape the word cloud. You need to provide the path to an image file (e.g., a PNG file with a transparent background).\n",
        "\n",
        "3. **Advanced Styling:**\n",
        "   - `contour_width` and `contour_color` add contour lines to the word cloud.\n",
        "   - `random_state=42` ensures reproducibility by setting a random state.\n",
        "   - `max_words=200` limits the number of words displayed in the word cloud.\n",
        "   - `stopwords=stopwords` removes common stop words from the word cloud.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfLB7VUiVd-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get font:"
      ],
      "metadata": {
        "id": "sNmiVxhZtUYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1HwUK9hixOUcF_8mrpP2nD4zbyMA0utz2'\n",
        "output = 'Arabic_font.ttf'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "WA8S-G_ptOgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Image:"
      ],
      "metadata": {
        "id": "TlJJNqMgtkN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1WoIBqqLufm9Ou3toI5uKVIcRuC-_Qswh'\n",
        "output = 'Mic.png'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "AJsLljYKtjuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def visualize_word_cloud(texts, mask_path, font_path):\n",
        "    # Combine all texts into one string\n",
        "    all_words = ' '.join(texts)\n",
        "\n",
        "    # Load a mask image for custom shape\n",
        "    mask = np.array(Image.open(mask_path))  # mask image path\n",
        "\n",
        "    # Define stop words\n",
        "    stopwords = set(STOPWORDS)\n",
        "\n",
        "    # Create a WordCloud object with additional customization\n",
        "    wordcloud = WordCloud(\n",
        "        font_path=font_path,\n",
        "        background_color='white',\n",
        "        max_words=200,\n",
        "        stopwords=stopwords,\n",
        "        mask=mask,  # Use mask for custom shape\n",
        "        contour_width=3,\n",
        "        contour_color='firebrick',\n",
        "        colormap='viridis',  # Color theme\n",
        "        width=800,\n",
        "        height=400,\n",
        "        random_state=42\n",
        "    ).generate(all_words)\n",
        "\n",
        "    # Display the word cloud\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_word_cloud(balanced_df['clean_text'], 'Mic.png', 'Arabic_font.ttf')\n"
      ],
      "metadata": {
        "id": "LlxaWDoOU6lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import mglearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('BalancedArabicNewsDataset.csv', on_bad_lines='skip')\n",
        "\n",
        "# Check for null values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Fill or drop null values\n",
        "df['description'] = df['description'].fillna('')\n",
        "df['date_publish'] = df['date_publish'].fillna('Unknown')\n",
        "\n",
        "# Use 'text' column for content and 'date_publish' or another column for labels\n",
        "df['text'] = df['text'] + ' ' + df['description']  # Combining text and description if needed\n",
        "df = df[['date_publish', 'text']]\n",
        "df.columns = ['year', 'abstract']\n",
        "\n",
        "# Ensure no null abstracts\n",
        "df = df[pd.notnull(df['abstract'])]\n",
        "\n",
        "# Create categories (labels) from the year column\n",
        "df['category_id'] = df['year'].factorize()[0]\n",
        "category_id_df = df[['year', 'category_id']].drop_duplicates().sort_values('category_id')\n",
        "category_to_id = dict(category_id_df.values)\n",
        "id_to_category = dict(category_id_df[['category_id', 'year']].values)\n",
        "df.head()\n",
        "\n",
        "# Define models to use\n",
        "modelsArray = [\"SVM\", \"NB\", \"LR\"]\n",
        "for model_type in modelsArray:\n",
        "    # Prepare the training and testing dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df['abstract'], df['year'], random_state=1, test_size=0.30)\n",
        "\n",
        "    # Vectorization\n",
        "    count_vect = TfidfVectorizer(analyzer='word', ngram_range=(1, 1))\n",
        "    count_vect.fit(X_train)\n",
        "    X_train_tfidf = count_vect.transform(X_train)\n",
        "    X_test_tfidf = count_vect.transform(X_test)\n",
        "\n",
        "    # Algorithms setup\n",
        "    if model_type == \"SVM\":\n",
        "        clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "    if model_type == \"NB\":\n",
        "        clf = MultinomialNB()\n",
        "    if model_type == \"LR\":\n",
        "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=4000)\n",
        "\n",
        "    # Train the model\n",
        "    train_model = clf.fit(X_train_tfidf, y_train)\n",
        "    # Predicting years for testing data\n",
        "    test_accuracy = train_model.predict(X_test_tfidf)\n",
        "    # Print training and testing accuracy\n",
        "    print(\"Training/Testing Accuracy\", '\\t', model_type, '\\t', train_model.score(X_train_tfidf, y_train), '\\t', train_model.score(X_test_tfidf, y_test))\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    conf_mat = confusion_matrix(y_test, test_accuracy)\n",
        "    fig, ax = plt.subplots(figsize=(9, 9))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"RdBu_r\",\n",
        "                xticklabels=category_id_df.year.values, yticklabels=category_id_df.year.values)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "    # Save plot\n",
        "    pltFileName = f'combined_{model_type}.pdf'\n",
        "    plt.savefig(pltFileName)\n"
      ],
      "metadata": {
        "id": "gQ9GumTzZiZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now use what you learnet to visualise and explore more"
      ],
      "metadata": {
        "id": "QmCX4RHkvC2X"
      }
    }
  ]
}